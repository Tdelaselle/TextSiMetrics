{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f782f76-c571-4c09-92b4-0e7b51cb9467",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Auguste 2024\n",
    "@author: ThÃ©otime de la Selle\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import nltk as nl\n",
    "import pylcs\n",
    "\n",
    "import cltk\n",
    "from cltk import NLP\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea061d3b-fa72-431d-8cef-d6d226c6d5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dirpath = \"data/SBLGNT/\"\n",
    "dirpath = \"data/LXX-Rahlf-1935/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92282078-8154-49cd-a6b4-271f6e8fc092",
   "metadata": {},
   "source": [
    "# Load and verify data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbc97c7b-8b7c-45e9-86b2-98483204976c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load (reading csv file) and format data\n",
    "def load_bible_texts(path):\n",
    "    df = pd.read_csv(path,delimiter='\\t', skiprows=(0),dtype=str,header=1)\n",
    "    df_rows = df.shape[0]\n",
    "    return df\n",
    "\n",
    "# def unaccented_text(pd):\n",
    "#     pd_unaccented = pd\n",
    "#     for i in tqdm(range(pd.shape[0])):\n",
    "#         pd_unaccented.text[i] = strip_accents(pd.text[i])\n",
    "#     return pd_unaccented\n",
    "\n",
    "# For loading multiple files \n",
    "# filepaths = list(filter(os.path.isfile, glob.glob(dirpath+\"*.txt\")))\n",
    "# filepaths.sort()\n",
    "\n",
    "# For loading one file\n",
    "filepaths = [dirpath+\"LXX.txt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb238822-aaa2-4a18-8806-9c1d05392e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>verse</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6009</th>\n",
       "      <td>ã€ŒJoshB 9:26ã€</td>\n",
       "      <td>ÎºÎ±á½¶ á¼Ï€Î¿Î¯Î·ÏƒÎ±Î½ Î±á½Ï„Î¿á¿–Ï‚ Î¿á½•Ï„Ï‰Ï‚ ÎºÎ±á½¶ á¼Î¾ÎµÎ¯Î»Î±Ï„Î¿ Î±á½Ï„Î¿á½ºÏ‚ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19656</th>\n",
       "      <td>ã€ŒOd 5:16ã€</td>\n",
       "      <td>ÎºÏÏÎ¹Îµ á¼Î½ Î¸Î»Î¯ÏˆÎµÎ¹ á¼Î¼Î½Î®ÏƒÎ¸Î·Î½ ÏƒÎ¿Ï… á¼Î½ Î¸Î»Î¯ÏˆÎµÎ¹ Î¼Î¹ÎºÏá¾· á¼¡...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8951</th>\n",
       "      <td>ã€Œ2Sam/K 13:7ã€</td>\n",
       "      <td>ÎºÎ±á½¶ á¼€Ï€Î­ÏƒÏ„ÎµÎ¹Î»ÎµÎ½ Î”Î±Ï…Î¹Î´ Ï€Ïá½¸Ï‚ Î˜Î·Î¼Î±Ï Îµá¼°Ï‚ Ï„á½¸Î½ Î¿á¼¶ÎºÎ¿Î½ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1850</th>\n",
       "      <td>ã€ŒExod 12:36ã€</td>\n",
       "      <td>ÎºÎ±á½¶ ÎºÏÏÎ¹Î¿Ï‚ á¼”Î´Ï‰ÎºÎµÎ½ Ï„á½´Î½ Ï‡Î¬ÏÎ¹Î½ Ï„á¿· Î»Î±á¿· Î±á½Ï„Î¿á¿¦ á¼Î½Î±Î½Ï„...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15628</th>\n",
       "      <td>ã€Œ1Mac 13:38ã€</td>\n",
       "      <td>ÎºÎ±á½¶ á½…ÏƒÎ± á¼ÏƒÏ„Î®ÏƒÎ±Î¼ÎµÎ½ Ï€Ïá½¸Ï‚ á½‘Î¼á¾¶Ï‚ á¼•ÏƒÏ„Î·ÎºÎµÎ½ ÎºÎ±á½¶ Ï„á½° á½€Ï‡Ï…...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               verse                                               text\n",
       "6009    ã€ŒJoshB 9:26ã€  ÎºÎ±á½¶ á¼Ï€Î¿Î¯Î·ÏƒÎ±Î½ Î±á½Ï„Î¿á¿–Ï‚ Î¿á½•Ï„Ï‰Ï‚ ÎºÎ±á½¶ á¼Î¾ÎµÎ¯Î»Î±Ï„Î¿ Î±á½Ï„Î¿á½ºÏ‚ ...\n",
       "19656      ã€ŒOd 5:16ã€  ÎºÏÏÎ¹Îµ á¼Î½ Î¸Î»Î¯ÏˆÎµÎ¹ á¼Î¼Î½Î®ÏƒÎ¸Î·Î½ ÏƒÎ¿Ï… á¼Î½ Î¸Î»Î¯ÏˆÎµÎ¹ Î¼Î¹ÎºÏá¾· á¼¡...\n",
       "8951   ã€Œ2Sam/K 13:7ã€  ÎºÎ±á½¶ á¼€Ï€Î­ÏƒÏ„ÎµÎ¹Î»ÎµÎ½ Î”Î±Ï…Î¹Î´ Ï€Ïá½¸Ï‚ Î˜Î·Î¼Î±Ï Îµá¼°Ï‚ Ï„á½¸Î½ Î¿á¼¶ÎºÎ¿Î½ ...\n",
       "1850    ã€ŒExod 12:36ã€  ÎºÎ±á½¶ ÎºÏÏÎ¹Î¿Ï‚ á¼”Î´Ï‰ÎºÎµÎ½ Ï„á½´Î½ Ï‡Î¬ÏÎ¹Î½ Ï„á¿· Î»Î±á¿· Î±á½Ï„Î¿á¿¦ á¼Î½Î±Î½Ï„...\n",
       "15628   ã€Œ1Mac 13:38ã€  ÎºÎ±á½¶ á½…ÏƒÎ± á¼ÏƒÏ„Î®ÏƒÎ±Î¼ÎµÎ½ Ï€Ïá½¸Ï‚ á½‘Î¼á¾¶Ï‚ á¼•ÏƒÏ„Î·ÎºÎµÎ½ ÎºÎ±á½¶ Ï„á½° á½€Ï‡Ï…..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30637\n"
     ]
    }
   ],
   "source": [
    "df = [load_bible_texts(filepaths[0])]\n",
    "for filepath in tqdm(filepaths[1:]):\n",
    "    df.append(load_bible_texts(filepath))\n",
    "\n",
    "NT = pd.concat(df)\n",
    "NT.reset_index(drop=True,inplace=True)\n",
    "NT_verses = NT.shape[0]\n",
    "\n",
    "display(NT.sample(5))\n",
    "print(NT_verses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344c2e12-d714-4240-8818-41d50daa3c36",
   "metadata": {},
   "source": [
    "# Data standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd4e6742-c473-41fb-b663-0b98a27b99ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2307     ÎºÎ±á½¶ ÎºÎ±Î¸Ï…Ï†Î±Î½Îµá¿–Ï‚ á¼Î½ Î±á½Ï„á¿· á½•Ï†Î±ÏƒÎ¼Î± ÎºÎ±Ï„Î¬Î»Î¹Î¸Î¿Î½ Ï„ÎµÏ„ÏÎ¬Ïƒ...\n",
       "1373     Îµá¼¶Ï€ÎµÎ½ Î´á½² Ï†Î±ÏÎ±Ï‰ Ï€Ïá½¸Ï‚ Î¹Ï‰ÏƒÎ·Ï† Îµá¼°Ï€á½¸Î½ Ï„Î¿á¿–Ï‚ á¼€Î´ÎµÎ»Ï†Î¿á¿–Ï‚ ...\n",
       "18541    Î¿á½ Ï†Î¿Î²Î·Î¸Î®Ïƒá¿ƒ á¼€Ï€á½¸ Ï†ÏŒÎ²Î¿Ï… Î½Ï…ÎºÏ„ÎµÏÎ¹Î½Î¿á¿¦ á¼€Ï€á½¸ Î²Î­Î»Î¿Ï…Ï‚ Ï€Îµ...\n",
       "30371    Ï„ÏŒÏ„Îµ Î´Î±ÏÎµá¿–Î¿Ï‚ á½ Î²Î±ÏƒÎ¹Î»Îµá½ºÏ‚ á¼”Î³ÏÎ±ÏˆÎµÎ½ Ï€á¾¶ÏƒÎ¹ Ï„Î¿á¿–Ï‚ Î»Î±Î¿á¿–...\n",
       "10839    ÎºÎ±á½¶ á¼Î²Î±ÏƒÎ¯Î»ÎµÏ…ÏƒÎµÎ½ Ï†Î±ÏÎ±Ï‰ Î½ÎµÏ‡Î±Ï‰ á¼Ï€ Ì“ Î±á½Ï„Î¿á½ºÏ‚ Ï„á½¸Î½ ÎµÎ»...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def standardize(df):\n",
    "    \n",
    "    # Normalize greek text and remove critical apparatus characters\n",
    "    from cltk.alphabet.grc import normalize_grc\n",
    "    from cltk.alphabet.grc import tonos_oxia_converter\n",
    "    from cltk.alphabet.grc import drop_critical_apparatus_char\n",
    "    from cltk.alphabet.grc import filter_non_greek\n",
    "    from cltk.alphabet.grc import expand_iota_subscript\n",
    "    # df.text = [expand_iota_subscript(txt) for txt in df.text]\n",
    "    df.text = [drop_critical_apparatus_char(txt) for txt in df.text]\n",
    "    df.text = [filter_non_greek(txt) for txt in df.text]\n",
    "    # df.text = [tonos_oxia_converter(txt) for txt in df.text]\n",
    "    df.text = [normalize_grc(txt) for txt in df.text]\n",
    "\n",
    "    # Manually remove critical apparatus symbols\n",
    "    # crit_symbols_list = ['â¸€', 'â¸‚','â¸ƒ','âŸ§','âŸ¦'] # list of symbols\n",
    "    # df.text = df.text.replace(crit_symbols_list,'',regex=True)\n",
    "    \n",
    "    # Lower case\n",
    "    df.text = df.text.str.lower()\n",
    "    return df\n",
    "\n",
    "NT = standardize(NT)\n",
    "\n",
    "# Control\n",
    "NT.text.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7449bbc-239d-45c5-8b63-7e67589e9797",
   "metadata": {},
   "source": [
    "# Cltk pipeline application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "217c2fd4-aa83-40fd-aff8-5dc5184fd736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â€ğ¤€ CLTK version '1.3.0'. When using the CLTK in research, please cite: https://aclanthology.org/2021.acl-demo.3/\n",
      "\n",
      "Pipeline for language 'Ancient Greek' (ISO: 'grc'): `GreekNormalizeProcess`, `GreekSpacyProcess`, `GreekEmbeddingsProcess`, `StopsProcess`.\n",
      "\n",
      "â¸– ``GreekSpacyProcess`` using OdyCy model by Center for Humanities Computing Aarhus from https://huggingface.co/chcaa . Please cite: https://aclanthology.org/2023.latechclfl-1.14\n",
      "â¸– ``LatinEmbeddingsProcess`` using word2vec model by University of Oslo from http://vectors.nlpl.eu/ . Please cite: https://aclanthology.org/W17-0237/\n",
      "\n",
      "â¸ To suppress these messages, instantiate ``NLP()`` with ``suppress_banner=True``.\n"
     ]
    }
   ],
   "source": [
    "cltk_nlp_grc = NLP(language=\"grc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbe9050b-885f-41ad-aa89-1cf77c1332a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NT pipeline application: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30637/30637 [05:10<00:00, 98.80it/s] \n"
     ]
    }
   ],
   "source": [
    "NT_doc = [cltk_nlp_grc.analyze(text=NT.text[i]) for i in tqdm(range(NT_verses),desc=\"NT pipeline application\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2df8ce-9246-4471-ab6c-6f97e04ddf07",
   "metadata": {},
   "source": [
    "# Dataframe pre-processing from Cltk doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83e9294d-f88a-418f-86ac-f9b844367fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of the additionnal stop words list (especially for lemmata)\n",
    "added_stop_words = ['Î´Îµá¿–','á½§Î´Îµ','á¼Î³Ï','á¼•Ï‰Ï‚','á¼€Î»Î»','á¼Î¬Î½','á¼•Î¾','ÎºÎ±Ï„Î¬','ÎºÎ±Î¯','Î±Ï…Ì“Ï„ÏŒÏ‚','Î¼ÎµÏ„Î¬','Î±á½Ï„á½¸Î½', 'Îµá½Î¸ÏÏ‚','ÏƒÏ', \"Ï„ÏŒÏ„Îµ\",\"Ï€á¾¶ÏƒÎ±\",\"Ï€á¾¶Ï‚\",\"á¼µÎ½Î±\",\"á½…Ï‚\",\"Ï„Î¯Ï‚\",\"Ï„Î¹Ï‚\",\"á¼€Ï€ÏŒ\",\"Î¼Î®\",'Ï„á¿¶Î¹','á½‘Ï€',\"Ï€á¿¶Ï‚\",\"á½…Ï„Î±Î½\",'á¼Ï€Î¯',\"Î´\",\"Îµá¼·Ï‚\",\"Î¿á½—Ï„Î¿Ï‚\",\"Ï€ÏÏŒÏ‚\",\"Ï€Ïá½¸Ï‚\",\"Ï€ÏÏŒ\",\"Î¿á½–Ï‚\",\"á½…Ï„Îµ\",\"Î³Î¬Ï\",\"Î´Î­\",\"Ï€á¾¶Ï‚\"]\n",
    "# + 'Îµá¼¶Î¼Î¹' + 'Îµá½–' + 'Ï€Î¿á¿¦' ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46c49ace-273c-4087-81d3-21546d4c31c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Processing of dataframe ---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokens: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30637/30637 [00:00<00:00, 150286.23it/s]\n",
      "Lemmata: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30637/30637 [00:00<00:00, 183023.63it/s]\n",
      "Tokens filtered: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30637/30637 [00:00<00:00, 114724.13it/s]\n",
      "Lemmata filtered: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30637/30637 [00:00<00:00, 43888.46it/s]\n",
      "Bigrams: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30637/30637 [00:00<00:00, 244886.28it/s]\n",
      "Trigrams: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30637/30637 [00:00<00:00, 235790.54it/s]\n",
      "Part-of-Speech: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30637/30637 [00:00<00:00, 135775.76it/s]\n",
      "Morphosyntactic features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30637/30637 [00:00<00:00, 203771.40it/s]\n",
      "Vocabulary: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30637/30637 [00:00<00:00, 50146.32it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>verse</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemmata</th>\n",
       "      <th>tokens_filtered</th>\n",
       "      <th>lemmata_filtered</th>\n",
       "      <th>bigrams</th>\n",
       "      <th>trigrams</th>\n",
       "      <th>pos</th>\n",
       "      <th>morpho</th>\n",
       "      <th>vocabulary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21148</th>\n",
       "      <td>ã€ŒJob 2:6ã€</td>\n",
       "      <td>Îµá¼¶Ï€ÎµÎ½ Î´á½² á½ ÎºÏÏÎ¹Î¿Ï‚ Ï„á¿· Î´Î¹Î±Î²ÏŒÎ»á¿³ á¼°Î´Î¿á½º Ï€Î±ÏÎ±Î´Î¯Î´Ï‰Î¼Î¯ Ïƒ...</td>\n",
       "      <td>[Îµá¼¶Ï€ÎµÎ½, Î´á½², á½, ÎºÏÏÎ¹Î¿Ï‚, Ï„á¿·, Î´Î¹Î±Î²ÏŒÎ»á¿³, á¼°Î´Î¿á½º, Ï€Î±ÏÎ±...</td>\n",
       "      <td>[Î»Î­Î³Ï‰, Î´Î­, á½, ÎºÏÏÎ¹Î¿Ï‚, á½, Î´Î¹Î¬Î²Î¿Î»Î¿Ï‚, á¼°Î´Î¿á½º, Ï€Î±ÏÎ±Î´...</td>\n",
       "      <td>[Îµá¼¶Ï€ÎµÎ½, ÎºÏÏÎ¹Î¿Ï‚, Î´Î¹Î±Î²ÏŒÎ»á¿³, á¼°Î´Î¿á½º, Ï€Î±ÏÎ±Î´Î¯Î´Ï‰Î¼Î¯, ÏƒÎ¿Î¹...</td>\n",
       "      <td>[Îµá¼¶Ï€Î¿Î½, ÎºÏÏÎ¹Î¿Ï‚, Î´Î¹Î¬Î²Î¿Î»Î¿Ï‚, á¼°Î´Î¿Ï, Ï€Î±ÏÎ±Î´Î¯Î´Ï‰Î¼Î¯, Î±á½...</td>\n",
       "      <td>[(Îµá¼¶Ï€Î¿Î½, ÎºÏÏÎ¹Î¿Ï‚), (ÎºÏÏÎ¹Î¿Ï‚, Î´Î¹Î¬Î²Î¿Î»Î¿Ï‚), (Î´Î¹Î¬Î²Î¿Î»Î¿...</td>\n",
       "      <td>[(Îµá¼¶Ï€Î¿Î½, ÎºÏÏÎ¹Î¿Ï‚, Î´Î¹Î¬Î²Î¿Î»Î¿Ï‚), (ÎºÏÏÎ¹Î¿Ï‚, Î´Î¹Î¬Î²Î¿Î»Î¿Ï‚,...</td>\n",
       "      <td>[verb, adverb, determiner, noun, determiner, n...</td>\n",
       "      <td>[[(habitual, imperfective, iterative, perfecti...</td>\n",
       "      <td>[Î±á½Ï„ÏŒÏ‚, Î´Î¹Î¬Î²Î¿Î»Î¿Ï‚, Î´Î¹Î±Ï†Ï…Î»Î¬ÏƒÏƒÏ‰, Îµá¼¶Ï€Î¿Î½, ÎºÏÏÎ¹Î¿Ï‚, Î¼...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11198</th>\n",
       "      <td>ã€Œ1Chr 7:35ã€</td>\n",
       "      <td>ÎºÎ±á½¶ Î²Î±Î½Î·ÎµÎ»Î±Î¼ á¼€Î´ÎµÎ»Ï†Î¿á½¶ Î±á½Ï„Î¿á¿¦ ÏƒÏ‰Ï†Î± ÎºÎ±á½¶ Î¹Î¼Î±Î½Î± ÎºÎ±á½¶ ...</td>\n",
       "      <td>[ÎºÎ±á½¶, Î²Î±Î½Î·ÎµÎ»Î±Î¼, á¼€Î´ÎµÎ»Ï†Î¿á½¶, Î±á½Ï„Î¿á¿¦, ÏƒÏ‰Ï†Î±, ÎºÎ±á½¶, Î¹Î¼Î±...</td>\n",
       "      <td>[ÎºÎ±Î¯, Î²Î±Î½Î·ÎµÎ»Î±Î¼, á¼€Î´ÎµÎ»Ï†ÏŒÏ‚, Î±á½Ï„ÏŒÏ‚, ÏƒÏ‰Ï†Î¿Ï‚, ÎºÎ±Î¯, Î¹Î¼...</td>\n",
       "      <td>[Î²Î±Î½Î·ÎµÎ»Î±Î¼, á¼€Î´ÎµÎ»Ï†Î¿á½¶, Î±á½Ï„Î¿á¿¦, ÏƒÏ‰Ï†Î±, Î¹Î¼Î±Î½Î±, ÏƒÎµÎ»Î»Î·Ï‚...</td>\n",
       "      <td>[Î²Î±Î½Î·ÎµÎ»Î±Î¼, á¼€Î´ÎµÎ»Ï†ÏŒÏ‚, Î±á½Ï„ÏŒÏ‚, ÏƒÏ‰Ï†Î±, Î¹Î¼Î±Î½Î±, ÏƒÎµÎ»Î»Î·Ï‚...</td>\n",
       "      <td>[(Î²Î±Î½Î·ÎµÎ»Î±Î¼, á¼€Î´ÎµÎ»Ï†ÏŒÏ‚), (á¼€Î´ÎµÎ»Ï†ÏŒÏ‚, Î±á½Ï„ÏŒÏ‚), (Î±á½Ï„ÏŒÏ‚...</td>\n",
       "      <td>[(Î²Î±Î½Î·ÎµÎ»Î±Î¼, á¼€Î´ÎµÎ»Ï†ÏŒÏ‚, Î±á½Ï„ÏŒÏ‚), (á¼€Î´ÎµÎ»Ï†ÏŒÏ‚, Î±á½Ï„ÏŒÏ‚, ...</td>\n",
       "      <td>[coordinating_conjunction, noun, noun, pronoun...</td>\n",
       "      <td>[[], [(nominative, accusative, ergative, absol...</td>\n",
       "      <td>[Î±Î¼Î±Î», Î±á½Ï„ÏŒÏ‚, Î²Î±Î½Î·ÎµÎ»Î±Î¼, Î¹Î¼Î±Î½Î±, ÏƒÎµÎ»Î»Î·Ï‚, ÏƒÏ‰Ï†Î±, á¼€...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3352</th>\n",
       "      <td>ã€ŒLev 22:33ã€</td>\n",
       "      <td>á½ á¼Î¾Î±Î³Î±Î³á½¼Î½ á½‘Î¼á¾¶Ï‚ á¼Îº Î³á¿†Ï‚ Î±á¼°Î³ÏÏ€Ï„Î¿Ï… á½¥ÏƒÏ„Îµ Îµá¼¶Î½Î±Î¹ á½‘Î¼á¿¶...</td>\n",
       "      <td>[á½, á¼Î¾Î±Î³Î±Î³á½¼Î½, á½‘Î¼á¾¶Ï‚, á¼Îº, Î³á¿†Ï‚, Î±á¼°Î³ÏÏ€Ï„Î¿Ï…, á½¥ÏƒÏ„Îµ, Îµ...</td>\n",
       "      <td>[á½, á¼Î¾Î¬Î³Ï‰, á½‘Î¼Îµá¿–Ï‚, á¼Îº, Î³á¿†, Î±á¼´Î³Ï…Ï€Ï„Î¿Ï‚, á½¥ÏƒÏ„Îµ, Îµá¼°Î¼Î¯...</td>\n",
       "      <td>[á¼Î¾Î±Î³Î±Î³á½¼Î½, á½‘Î¼á¾¶Ï‚, Î³á¿†Ï‚, Î±á¼°Î³ÏÏ€Ï„Î¿Ï…, Îµá¼¶Î½Î±Î¹, á½‘Î¼á¿¶Î½, Î¸...</td>\n",
       "      <td>[á¼Î¾Î¬Î³Ï‰, Î³á¿†, Î±á¼´Î³Ï…Ï€Ï„Î¿Ï‚, Îµá¼°Î¼Î¯, Î¸ÎµÏŒÏ‚, ÎºÏÏÎ¹Î¿Ï‚]</td>\n",
       "      <td>[(á¼Î¾Î¬Î³Ï‰, Î³á¿†), (Î³á¿†, Î±á¼´Î³Ï…Ï€Ï„Î¿Ï‚), (Î±á¼´Î³Ï…Ï€Ï„Î¿Ï‚, Îµá¼°Î¼Î¯)...</td>\n",
       "      <td>[(á¼Î¾Î¬Î³Ï‰, Î³á¿†, Î±á¼´Î³Ï…Ï€Ï„Î¿Ï‚), (Î³á¿†, Î±á¼´Î³Ï…Ï€Ï„Î¿Ï‚, Îµá¼°Î¼Î¯), ...</td>\n",
       "      <td>[determiner, verb, pronoun, adposition, noun, ...</td>\n",
       "      <td>[[(nominative, accusative, ergative, absolutiv...</td>\n",
       "      <td>[Î±á¼´Î³Ï…Ï€Ï„Î¿Ï‚, Î³á¿†, Îµá¼°Î¼Î¯, Î¸ÎµÏŒÏ‚, ÎºÏÏÎ¹Î¿Ï‚, á¼Î¾Î¬Î³Ï‰]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9325</th>\n",
       "      <td>ã€Œ2Sam/K 24:6ã€</td>\n",
       "      <td>ÎºÎ±á½¶ á¼¦Î»Î¸Î¿Î½ Îµá¼°Ï‚ Ï„á½´Î½ Î³Î±Î»Î±Î±Î´ ÎºÎ±á½¶ Îµá¼°Ï‚ Î³á¿†Î½ Î¸Î±Î²Î±ÏƒÏ‰Î½ á¼¥...</td>\n",
       "      <td>[ÎºÎ±á½¶, á¼¦Î»Î¸Î¿Î½, Îµá¼°Ï‚, Ï„á½´Î½, Î³Î±Î»Î±Î±Î´, ÎºÎ±á½¶, Îµá¼°Ï‚, Î³á¿†Î½, ...</td>\n",
       "      <td>[ÎºÎ±Î¯, á¼”ÏÏ‡Î¿Î¼Î±Î¹, Îµá¼°Ï‚, á½, Î³Î±Î»Î±Î±Î´, ÎºÎ±Î¯, Îµá¼°Ï‚, Î³á¿†, Î¸...</td>\n",
       "      <td>[á¼¦Î»Î¸Î¿Î½, Î³Î±Î»Î±Î±Î´, Î³á¿†Î½, Î¸Î±Î²Î±ÏƒÏ‰Î½, á¼ÏƒÏ„Î¹Î½, Î±Î´Î±ÏƒÎ±Î¹, Ï€...</td>\n",
       "      <td>[á¼”ÏÏ‡Î¿Î¼Î±Î¹, Î³Î±Î»Î±Î±Î´, Î³á¿†, Î¸Î±Î²Î±ÏƒÏ‰Î½, Îµá¼°Î¼Î¯, Î±Î´Î±ÏƒÎ±Î¹, Ï€...</td>\n",
       "      <td>[(á¼”ÏÏ‡Î¿Î¼Î±Î¹, Î³Î±Î»Î±Î±Î´), (Î³Î±Î»Î±Î±Î´, Î³á¿†), (Î³á¿†, Î¸Î±Î²Î±ÏƒÏ‰Î½...</td>\n",
       "      <td>[(á¼”ÏÏ‡Î¿Î¼Î±Î¹, Î³Î±Î»Î±Î±Î´, Î³á¿†), (Î³Î±Î»Î±Î±Î´, Î³á¿†, Î¸Î±Î²Î±ÏƒÏ‰Î½),...</td>\n",
       "      <td>[coordinating_conjunction, verb, adposition, d...</td>\n",
       "      <td>[[], [(habitual, imperfective, iterative, perf...</td>\n",
       "      <td>[Î±Î´Î±ÏƒÎ±Î¹, Î³Î±Î»Î±Î±Î´, Î³á¿†, Î´Î±Î½Î¹Î´Î±Î½, Îµá¼°Î¼Î¯, Î¸Î±Î²Î±ÏƒÏ‰Î½, Îº...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21434</th>\n",
       "      <td>ã€ŒJob 13:25ã€</td>\n",
       "      <td>á¼¦ á½¡Ï‚ Ï†ÏÎ»Î»Î¿Î½ ÎºÎ¹Î½Î¿ÏÎ¼ÎµÎ½Î¿Î½ á½‘Ï€á½¸ á¼€Î½Î­Î¼Î¿Ï… Îµá½Î»Î±Î²Î·Î¸Î®Ïƒá¿ƒ á¼¢...</td>\n",
       "      <td>[á¼¦, á½¡Ï‚, Ï†ÏÎ»Î»Î¿Î½, ÎºÎ¹Î½Î¿ÏÎ¼ÎµÎ½Î¿Î½, á½‘Ï€á½¸, á¼€Î½Î­Î¼Î¿Ï…, Îµá½Î»Î±Î²...</td>\n",
       "      <td>[á¼¦, á½¡Ï‚, Ï†ÏÎ»Î»Î¿Ï‚, ÎºÎ¹Î½Î­Ï‰, á½‘Ï€ÏŒ, á¼„Î½ÎµÎ¼Î¿Ï‚, Îµá½Î»Î±Î²Î·Î¸Î­Ï‰,...</td>\n",
       "      <td>[á¼¦, Ï†ÏÎ»Î»Î¿Î½, ÎºÎ¹Î½Î¿ÏÎ¼ÎµÎ½Î¿Î½, á¼€Î½Î­Î¼Î¿Ï…, Îµá½Î»Î±Î²Î·Î¸Î®Ïƒá¿ƒ, Ï‡ÏŒ...</td>\n",
       "      <td>[Îµá¼°Î¼Î¯, Ï†ÏÎ»Î»Î¿Î½, ÎºÎ¹Î½Î­Ï‰, á¼„Î½ÎµÎ¼Î¿Ï‚, ÎµÏ…Ì“Î»Î±Î²Î­Î¿Î¼Î±Î¹, Ï‡ÏŒÏ...</td>\n",
       "      <td>[(Îµá¼°Î¼Î¯, Ï†ÏÎ»Î»Î¿Î½), (Ï†ÏÎ»Î»Î¿Î½, ÎºÎ¹Î½Î­Ï‰), (ÎºÎ¹Î½Î­Ï‰, á¼„Î½ÎµÎ¼...</td>\n",
       "      <td>[(Îµá¼°Î¼Î¯, Ï†ÏÎ»Î»Î¿Î½, ÎºÎ¹Î½Î­Ï‰), (Ï†ÏÎ»Î»Î¿Î½, ÎºÎ¹Î½Î­Ï‰, á¼„Î½ÎµÎ¼Î¿Ï‚...</td>\n",
       "      <td>[adverb, adverb, adjective, verb, adposition, ...</td>\n",
       "      <td>[[], [], [(nominative, accusative, ergative, a...</td>\n",
       "      <td>[ÎµÏ…Ì“Î»Î±Î²Î­Î¿Î¼Î±Î¹, Îµá¼°Î¼Î¯, ÎºÎ¹Î½Î­Ï‰, Ï€Î½Îµá¿¦Î¼Î±, Ï†Î­ÏÏ‰, Ï†ÏÎ»Î»Î¿...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               verse                                               text  \\\n",
       "21148      ã€ŒJob 2:6ã€  Îµá¼¶Ï€ÎµÎ½ Î´á½² á½ ÎºÏÏÎ¹Î¿Ï‚ Ï„á¿· Î´Î¹Î±Î²ÏŒÎ»á¿³ á¼°Î´Î¿á½º Ï€Î±ÏÎ±Î´Î¯Î´Ï‰Î¼Î¯ Ïƒ...   \n",
       "11198    ã€Œ1Chr 7:35ã€  ÎºÎ±á½¶ Î²Î±Î½Î·ÎµÎ»Î±Î¼ á¼€Î´ÎµÎ»Ï†Î¿á½¶ Î±á½Ï„Î¿á¿¦ ÏƒÏ‰Ï†Î± ÎºÎ±á½¶ Î¹Î¼Î±Î½Î± ÎºÎ±á½¶ ...   \n",
       "3352     ã€ŒLev 22:33ã€  á½ á¼Î¾Î±Î³Î±Î³á½¼Î½ á½‘Î¼á¾¶Ï‚ á¼Îº Î³á¿†Ï‚ Î±á¼°Î³ÏÏ€Ï„Î¿Ï… á½¥ÏƒÏ„Îµ Îµá¼¶Î½Î±Î¹ á½‘Î¼á¿¶...   \n",
       "9325   ã€Œ2Sam/K 24:6ã€  ÎºÎ±á½¶ á¼¦Î»Î¸Î¿Î½ Îµá¼°Ï‚ Ï„á½´Î½ Î³Î±Î»Î±Î±Î´ ÎºÎ±á½¶ Îµá¼°Ï‚ Î³á¿†Î½ Î¸Î±Î²Î±ÏƒÏ‰Î½ á¼¥...   \n",
       "21434    ã€ŒJob 13:25ã€  á¼¦ á½¡Ï‚ Ï†ÏÎ»Î»Î¿Î½ ÎºÎ¹Î½Î¿ÏÎ¼ÎµÎ½Î¿Î½ á½‘Ï€á½¸ á¼€Î½Î­Î¼Î¿Ï… Îµá½Î»Î±Î²Î·Î¸Î®Ïƒá¿ƒ á¼¢...   \n",
       "\n",
       "                                                  tokens  \\\n",
       "21148  [Îµá¼¶Ï€ÎµÎ½, Î´á½², á½, ÎºÏÏÎ¹Î¿Ï‚, Ï„á¿·, Î´Î¹Î±Î²ÏŒÎ»á¿³, á¼°Î´Î¿á½º, Ï€Î±ÏÎ±...   \n",
       "11198  [ÎºÎ±á½¶, Î²Î±Î½Î·ÎµÎ»Î±Î¼, á¼€Î´ÎµÎ»Ï†Î¿á½¶, Î±á½Ï„Î¿á¿¦, ÏƒÏ‰Ï†Î±, ÎºÎ±á½¶, Î¹Î¼Î±...   \n",
       "3352   [á½, á¼Î¾Î±Î³Î±Î³á½¼Î½, á½‘Î¼á¾¶Ï‚, á¼Îº, Î³á¿†Ï‚, Î±á¼°Î³ÏÏ€Ï„Î¿Ï…, á½¥ÏƒÏ„Îµ, Îµ...   \n",
       "9325   [ÎºÎ±á½¶, á¼¦Î»Î¸Î¿Î½, Îµá¼°Ï‚, Ï„á½´Î½, Î³Î±Î»Î±Î±Î´, ÎºÎ±á½¶, Îµá¼°Ï‚, Î³á¿†Î½, ...   \n",
       "21434  [á¼¦, á½¡Ï‚, Ï†ÏÎ»Î»Î¿Î½, ÎºÎ¹Î½Î¿ÏÎ¼ÎµÎ½Î¿Î½, á½‘Ï€á½¸, á¼€Î½Î­Î¼Î¿Ï…, Îµá½Î»Î±Î²...   \n",
       "\n",
       "                                                 lemmata  \\\n",
       "21148  [Î»Î­Î³Ï‰, Î´Î­, á½, ÎºÏÏÎ¹Î¿Ï‚, á½, Î´Î¹Î¬Î²Î¿Î»Î¿Ï‚, á¼°Î´Î¿á½º, Ï€Î±ÏÎ±Î´...   \n",
       "11198  [ÎºÎ±Î¯, Î²Î±Î½Î·ÎµÎ»Î±Î¼, á¼€Î´ÎµÎ»Ï†ÏŒÏ‚, Î±á½Ï„ÏŒÏ‚, ÏƒÏ‰Ï†Î¿Ï‚, ÎºÎ±Î¯, Î¹Î¼...   \n",
       "3352   [á½, á¼Î¾Î¬Î³Ï‰, á½‘Î¼Îµá¿–Ï‚, á¼Îº, Î³á¿†, Î±á¼´Î³Ï…Ï€Ï„Î¿Ï‚, á½¥ÏƒÏ„Îµ, Îµá¼°Î¼Î¯...   \n",
       "9325   [ÎºÎ±Î¯, á¼”ÏÏ‡Î¿Î¼Î±Î¹, Îµá¼°Ï‚, á½, Î³Î±Î»Î±Î±Î´, ÎºÎ±Î¯, Îµá¼°Ï‚, Î³á¿†, Î¸...   \n",
       "21434  [á¼¦, á½¡Ï‚, Ï†ÏÎ»Î»Î¿Ï‚, ÎºÎ¹Î½Î­Ï‰, á½‘Ï€ÏŒ, á¼„Î½ÎµÎ¼Î¿Ï‚, Îµá½Î»Î±Î²Î·Î¸Î­Ï‰,...   \n",
       "\n",
       "                                         tokens_filtered  \\\n",
       "21148  [Îµá¼¶Ï€ÎµÎ½, ÎºÏÏÎ¹Î¿Ï‚, Î´Î¹Î±Î²ÏŒÎ»á¿³, á¼°Î´Î¿á½º, Ï€Î±ÏÎ±Î´Î¯Î´Ï‰Î¼Î¯, ÏƒÎ¿Î¹...   \n",
       "11198  [Î²Î±Î½Î·ÎµÎ»Î±Î¼, á¼€Î´ÎµÎ»Ï†Î¿á½¶, Î±á½Ï„Î¿á¿¦, ÏƒÏ‰Ï†Î±, Î¹Î¼Î±Î½Î±, ÏƒÎµÎ»Î»Î·Ï‚...   \n",
       "3352   [á¼Î¾Î±Î³Î±Î³á½¼Î½, á½‘Î¼á¾¶Ï‚, Î³á¿†Ï‚, Î±á¼°Î³ÏÏ€Ï„Î¿Ï…, Îµá¼¶Î½Î±Î¹, á½‘Î¼á¿¶Î½, Î¸...   \n",
       "9325   [á¼¦Î»Î¸Î¿Î½, Î³Î±Î»Î±Î±Î´, Î³á¿†Î½, Î¸Î±Î²Î±ÏƒÏ‰Î½, á¼ÏƒÏ„Î¹Î½, Î±Î´Î±ÏƒÎ±Î¹, Ï€...   \n",
       "21434  [á¼¦, Ï†ÏÎ»Î»Î¿Î½, ÎºÎ¹Î½Î¿ÏÎ¼ÎµÎ½Î¿Î½, á¼€Î½Î­Î¼Î¿Ï…, Îµá½Î»Î±Î²Î·Î¸Î®Ïƒá¿ƒ, Ï‡ÏŒ...   \n",
       "\n",
       "                                        lemmata_filtered  \\\n",
       "21148  [Îµá¼¶Ï€Î¿Î½, ÎºÏÏÎ¹Î¿Ï‚, Î´Î¹Î¬Î²Î¿Î»Î¿Ï‚, á¼°Î´Î¿Ï, Ï€Î±ÏÎ±Î´Î¯Î´Ï‰Î¼Î¯, Î±á½...   \n",
       "11198  [Î²Î±Î½Î·ÎµÎ»Î±Î¼, á¼€Î´ÎµÎ»Ï†ÏŒÏ‚, Î±á½Ï„ÏŒÏ‚, ÏƒÏ‰Ï†Î±, Î¹Î¼Î±Î½Î±, ÏƒÎµÎ»Î»Î·Ï‚...   \n",
       "3352           [á¼Î¾Î¬Î³Ï‰, Î³á¿†, Î±á¼´Î³Ï…Ï€Ï„Î¿Ï‚, Îµá¼°Î¼Î¯, Î¸ÎµÏŒÏ‚, ÎºÏÏÎ¹Î¿Ï‚]   \n",
       "9325   [á¼”ÏÏ‡Î¿Î¼Î±Î¹, Î³Î±Î»Î±Î±Î´, Î³á¿†, Î¸Î±Î²Î±ÏƒÏ‰Î½, Îµá¼°Î¼Î¯, Î±Î´Î±ÏƒÎ±Î¹, Ï€...   \n",
       "21434  [Îµá¼°Î¼Î¯, Ï†ÏÎ»Î»Î¿Î½, ÎºÎ¹Î½Î­Ï‰, á¼„Î½ÎµÎ¼Î¿Ï‚, ÎµÏ…Ì“Î»Î±Î²Î­Î¿Î¼Î±Î¹, Ï‡ÏŒÏ...   \n",
       "\n",
       "                                                 bigrams  \\\n",
       "21148  [(Îµá¼¶Ï€Î¿Î½, ÎºÏÏÎ¹Î¿Ï‚), (ÎºÏÏÎ¹Î¿Ï‚, Î´Î¹Î¬Î²Î¿Î»Î¿Ï‚), (Î´Î¹Î¬Î²Î¿Î»Î¿...   \n",
       "11198  [(Î²Î±Î½Î·ÎµÎ»Î±Î¼, á¼€Î´ÎµÎ»Ï†ÏŒÏ‚), (á¼€Î´ÎµÎ»Ï†ÏŒÏ‚, Î±á½Ï„ÏŒÏ‚), (Î±á½Ï„ÏŒÏ‚...   \n",
       "3352   [(á¼Î¾Î¬Î³Ï‰, Î³á¿†), (Î³á¿†, Î±á¼´Î³Ï…Ï€Ï„Î¿Ï‚), (Î±á¼´Î³Ï…Ï€Ï„Î¿Ï‚, Îµá¼°Î¼Î¯)...   \n",
       "9325   [(á¼”ÏÏ‡Î¿Î¼Î±Î¹, Î³Î±Î»Î±Î±Î´), (Î³Î±Î»Î±Î±Î´, Î³á¿†), (Î³á¿†, Î¸Î±Î²Î±ÏƒÏ‰Î½...   \n",
       "21434  [(Îµá¼°Î¼Î¯, Ï†ÏÎ»Î»Î¿Î½), (Ï†ÏÎ»Î»Î¿Î½, ÎºÎ¹Î½Î­Ï‰), (ÎºÎ¹Î½Î­Ï‰, á¼„Î½ÎµÎ¼...   \n",
       "\n",
       "                                                trigrams  \\\n",
       "21148  [(Îµá¼¶Ï€Î¿Î½, ÎºÏÏÎ¹Î¿Ï‚, Î´Î¹Î¬Î²Î¿Î»Î¿Ï‚), (ÎºÏÏÎ¹Î¿Ï‚, Î´Î¹Î¬Î²Î¿Î»Î¿Ï‚,...   \n",
       "11198  [(Î²Î±Î½Î·ÎµÎ»Î±Î¼, á¼€Î´ÎµÎ»Ï†ÏŒÏ‚, Î±á½Ï„ÏŒÏ‚), (á¼€Î´ÎµÎ»Ï†ÏŒÏ‚, Î±á½Ï„ÏŒÏ‚, ...   \n",
       "3352   [(á¼Î¾Î¬Î³Ï‰, Î³á¿†, Î±á¼´Î³Ï…Ï€Ï„Î¿Ï‚), (Î³á¿†, Î±á¼´Î³Ï…Ï€Ï„Î¿Ï‚, Îµá¼°Î¼Î¯), ...   \n",
       "9325   [(á¼”ÏÏ‡Î¿Î¼Î±Î¹, Î³Î±Î»Î±Î±Î´, Î³á¿†), (Î³Î±Î»Î±Î±Î´, Î³á¿†, Î¸Î±Î²Î±ÏƒÏ‰Î½),...   \n",
       "21434  [(Îµá¼°Î¼Î¯, Ï†ÏÎ»Î»Î¿Î½, ÎºÎ¹Î½Î­Ï‰), (Ï†ÏÎ»Î»Î¿Î½, ÎºÎ¹Î½Î­Ï‰, á¼„Î½ÎµÎ¼Î¿Ï‚...   \n",
       "\n",
       "                                                     pos  \\\n",
       "21148  [verb, adverb, determiner, noun, determiner, n...   \n",
       "11198  [coordinating_conjunction, noun, noun, pronoun...   \n",
       "3352   [determiner, verb, pronoun, adposition, noun, ...   \n",
       "9325   [coordinating_conjunction, verb, adposition, d...   \n",
       "21434  [adverb, adverb, adjective, verb, adposition, ...   \n",
       "\n",
       "                                                  morpho  \\\n",
       "21148  [[(habitual, imperfective, iterative, perfecti...   \n",
       "11198  [[], [(nominative, accusative, ergative, absol...   \n",
       "3352   [[(nominative, accusative, ergative, absolutiv...   \n",
       "9325   [[], [(habitual, imperfective, iterative, perf...   \n",
       "21434  [[], [], [(nominative, accusative, ergative, a...   \n",
       "\n",
       "                                              vocabulary  \n",
       "21148  [Î±á½Ï„ÏŒÏ‚, Î´Î¹Î¬Î²Î¿Î»Î¿Ï‚, Î´Î¹Î±Ï†Ï…Î»Î¬ÏƒÏƒÏ‰, Îµá¼¶Ï€Î¿Î½, ÎºÏÏÎ¹Î¿Ï‚, Î¼...  \n",
       "11198  [Î±Î¼Î±Î», Î±á½Ï„ÏŒÏ‚, Î²Î±Î½Î·ÎµÎ»Î±Î¼, Î¹Î¼Î±Î½Î±, ÏƒÎµÎ»Î»Î·Ï‚, ÏƒÏ‰Ï†Î±, á¼€...  \n",
       "3352           [Î±á¼´Î³Ï…Ï€Ï„Î¿Ï‚, Î³á¿†, Îµá¼°Î¼Î¯, Î¸ÎµÏŒÏ‚, ÎºÏÏÎ¹Î¿Ï‚, á¼Î¾Î¬Î³Ï‰]  \n",
       "9325   [Î±Î´Î±ÏƒÎ±Î¹, Î³Î±Î»Î±Î±Î´, Î³á¿†, Î´Î±Î½Î¹Î´Î±Î½, Îµá¼°Î¼Î¯, Î¸Î±Î²Î±ÏƒÏ‰Î½, Îº...  \n",
       "21434  [ÎµÏ…Ì“Î»Î±Î²Î­Î¿Î¼Î±Î¹, Îµá¼°Î¼Î¯, ÎºÎ¹Î½Î­Ï‰, Ï€Î½Îµá¿¦Î¼Î±, Ï†Î­ÏÏ‰, Ï†ÏÎ»Î»Î¿...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def text_dataframe_processing(df,doc):\n",
    "\n",
    "    print(\"-------- Processing of dataframe ---------\")  \n",
    "\n",
    "    # ----- Remove stop words and punctuation for adding filtered tokens and lemmata to dataframe\n",
    "    from cltk.stops.words import Stops\n",
    "    from cltk.text.processes import DefaultPunctuationRemovalProcess\n",
    "    from cltk.lemmatize.grc import GreekBackoffLemmatizer\n",
    "    \n",
    "    Punct_filter = DefaultPunctuationRemovalProcess(language='grc')\n",
    "    doc = [Punct_filter.run(d) for d in doc]\n",
    "    \n",
    "    # ----- Add tokens lemmata, and tokens_stops_filtered in dataframe\n",
    "    df['tokens'] = [d.tokens for d in tqdm(doc,desc=\"Tokens\")]\n",
    "    df['lemmata'] = [d.lemmata for d in tqdm(doc,desc=\"Lemmata\")]\n",
    "    df['tokens_filtered'] = [d.tokens_stops_filtered for d in tqdm(doc,desc=\"Tokens filtered\")]\n",
    "     \n",
    "    lemmatizer = GreekBackoffLemmatizer() # we have to lemmatize the filtered tokens\n",
    "    tokens_lemmata_filtered = [lemmatizer.lemmatize(lem) for lem in df.tokens_filtered]\n",
    "    lemmata_filtered = []\n",
    "    for lem in tokens_lemmata_filtered :    \n",
    "        lemmata_filtered.append([l[1] for l in lem])\n",
    "\n",
    "    # Removing stop words from lemmata (based on cltk.stops.words process but only through extra_stops additionnal list as remove_stopwords doesn't work for lemmata)\n",
    "    stops_obj = Stops(iso_code=\"grc\")\n",
    "    df['lemmata_filtered'] = [stops_obj.remove_stopwords(tokens=lem, extra_stops=added_stop_words) for lem in tqdm(lemmata_filtered,desc=\"Lemmata filtered\")]\n",
    "    \n",
    "    #----- Add n-grams\n",
    "    df['bigrams'] = [list(nl.bigrams(lem)) for lem in tqdm(df.lemmata_filtered,desc=\"Bigrams\")]\n",
    "    df['trigrams'] = [list(nl.trigrams(lem)) for lem in tqdm(df.lemmata_filtered,desc=\"Trigrams\")]\n",
    "\n",
    "    # ----- Add tf-idf score for each lemmata\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "    tfidf = TfidfVectorizer(\n",
    "        analyzer='word',\n",
    "        tokenizer=lambda x: x,\n",
    "        preprocessor=lambda x: x,\n",
    "        token_pattern=None)  \n",
    "\n",
    "    # Learn vocabulary and idf, return document-term matrix. \n",
    "    # doc_term_matrix = tfidf.fit_transform(df.lemmata_filtered)\n",
    "    # tfidf_values = [doc_term_matrix[i,j] for i, j in zip(*doc_term_matrix.nonzero())]\n",
    "    # id_tfidf = [i for i, j in zip(*doc_term_matrix.nonzero())]\n",
    "    # tfidf_verses = []\n",
    "    # for i in tqdm(range(df.shape[0]),desc=\"tfidf\"):\n",
    "    #     tfidf_verses.append([tfidf_values[index] for (index, item) in enumerate(id_tfidf) if item == i])\n",
    "    # df['lemmata_tfidf'] = [tf for tf in tfidf_verses]\n",
    "\n",
    "    # ---- Add part-of-speech feature in dataframe\n",
    "    pos = []\n",
    "    for i in tqdm(range(df.shape[0]),desc=\"Part-of-Speech\"):\n",
    "        pos.append([str(word.pos) for word in doc[i].words])   \n",
    "    df['pos'] = pos\n",
    "\n",
    "    # ---- Add morphosyntactic features in dataframe\n",
    "    df['morpho'] = [d.morphosyntactic_features for d in tqdm(doc,desc=\"Morphosyntactic features\")]    \n",
    "\n",
    "    # ----- Add vocabulary feature in dataframe\n",
    "    from nltk.lm import Vocabulary\n",
    "    vocab = []\n",
    "    for i in tqdm(range(df.shape[0]),desc=\"Vocabulary\"):\n",
    "        vocab.append(sorted(Vocabulary(df.lemmata_filtered[i], unk_cutoff=1).counts))\n",
    "    \n",
    "    df['vocabulary'] = vocab\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "Evangiles = text_dataframe_processing(NT,NT_doc)\n",
    "\n",
    "display(NT.sample(5)) # To controle pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47791728-d542-4b4d-8302-d9c8e2b9049f",
   "metadata": {},
   "source": [
    "# Metric test on 2 verses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5bf7ea3d-0d17-41a3-be62-a0deef649b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.metrics.distance import jaro_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de9a1fb5-6649-4464-b16f-5b809be61a38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>verse</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemmata</th>\n",
       "      <th>tokens_filtered</th>\n",
       "      <th>lemmata_filtered</th>\n",
       "      <th>bigrams</th>\n",
       "      <th>trigrams</th>\n",
       "      <th>pos</th>\n",
       "      <th>morpho</th>\n",
       "      <th>vocabulary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ã€ŒJudgA 13:1ã€</td>\n",
       "      <td>ÎºÎ±á½¶ Ï€ÏÎ¿ÏƒÎ­Î¸ÎµÎ½Ï„Î¿ Î¿á¼± Ï…á¼±Î¿á½¶ Î¹ÏƒÏÎ±Î·Î» Ï€Î¿Î¹á¿†ÏƒÎ±Î¹ Ï„á½¸ Ï€Î¿Î½Î·Ï...</td>\n",
       "      <td>[ÎºÎ±á½¶, Ï€ÏÎ¿ÏƒÎ­Î¸ÎµÎ½Ï„Î¿, Î¿á¼±, Ï…á¼±Î¿á½¶, Î¹ÏƒÏÎ±Î·Î», Ï€Î¿Î¹á¿†ÏƒÎ±Î¹, Ï„...</td>\n",
       "      <td>[ÎºÎ±Î¯, Ï€ÏÎ¿ÏƒÏ„Î¯Î¸Î·Î¼Î¹, á½, Ï…á¼±ÏŒÏ‚, Î¹ÏƒÏÎ±Î·Î», Ï€Î¿Î¹Î­Ï‰, á½, Ï€...</td>\n",
       "      <td>[Ï€ÏÎ¿ÏƒÎ­Î¸ÎµÎ½Ï„Î¿, Ï…á¼±Î¿á½¶, Î¹ÏƒÏÎ±Î·Î», Ï€Î¿Î¹á¿†ÏƒÎ±Î¹, Ï€Î¿Î½Î·Ïá½¸Î½, á¼...</td>\n",
       "      <td>[Ï€ÏÎ¿ÏƒÏ„Î¯Î¸Î·Î¼Î¹, Ï…á¼±Î¿á½¶, Î¹ÏƒÏÎ±Î·Î», Ï€Î¿Î¹Î­Ï‰, Ï€Î¿Î½Î·ÏÏŒÏ‚, á¼Î½Î±...</td>\n",
       "      <td>[(Ï€ÏÎ¿ÏƒÏ„Î¯Î¸Î·Î¼Î¹, Ï…á¼±Î¿á½¶), (Ï…á¼±Î¿á½¶, Î¹ÏƒÏÎ±Î·Î»), (Î¹ÏƒÏÎ±Î·Î», ...</td>\n",
       "      <td>[(Ï€ÏÎ¿ÏƒÏ„Î¯Î¸Î·Î¼Î¹, Ï…á¼±Î¿á½¶, Î¹ÏƒÏÎ±Î·Î»), (Ï…á¼±Î¿á½¶, Î¹ÏƒÏÎ±Î·Î», Ï€Î¿...</td>\n",
       "      <td>[coordinating_conjunction, verb, determiner, n...</td>\n",
       "      <td>[[], [(habitual, imperfective, iterative, perf...</td>\n",
       "      <td>[Î±á½Ï„ÏŒÏ‚, Î¹ÏƒÏÎ±Î·Î», ÎºÏÏÎ¹Î¿Ï‚, Ï€Î±ÏÎ±Î´Î¯Î´Ï‰Î¼Î¹, Ï€Î¿Î¹Î­Ï‰, Ï€Î¿Î½...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          verse                                               text  \\\n",
       "0  ã€ŒJudgA 13:1ã€  ÎºÎ±á½¶ Ï€ÏÎ¿ÏƒÎ­Î¸ÎµÎ½Ï„Î¿ Î¿á¼± Ï…á¼±Î¿á½¶ Î¹ÏƒÏÎ±Î·Î» Ï€Î¿Î¹á¿†ÏƒÎ±Î¹ Ï„á½¸ Ï€Î¿Î½Î·Ï...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [ÎºÎ±á½¶, Ï€ÏÎ¿ÏƒÎ­Î¸ÎµÎ½Ï„Î¿, Î¿á¼±, Ï…á¼±Î¿á½¶, Î¹ÏƒÏÎ±Î·Î», Ï€Î¿Î¹á¿†ÏƒÎ±Î¹, Ï„...   \n",
       "\n",
       "                                             lemmata  \\\n",
       "0  [ÎºÎ±Î¯, Ï€ÏÎ¿ÏƒÏ„Î¯Î¸Î·Î¼Î¹, á½, Ï…á¼±ÏŒÏ‚, Î¹ÏƒÏÎ±Î·Î», Ï€Î¿Î¹Î­Ï‰, á½, Ï€...   \n",
       "\n",
       "                                     tokens_filtered  \\\n",
       "0  [Ï€ÏÎ¿ÏƒÎ­Î¸ÎµÎ½Ï„Î¿, Ï…á¼±Î¿á½¶, Î¹ÏƒÏÎ±Î·Î», Ï€Î¿Î¹á¿†ÏƒÎ±Î¹, Ï€Î¿Î½Î·Ïá½¸Î½, á¼...   \n",
       "\n",
       "                                    lemmata_filtered  \\\n",
       "0  [Ï€ÏÎ¿ÏƒÏ„Î¯Î¸Î·Î¼Î¹, Ï…á¼±Î¿á½¶, Î¹ÏƒÏÎ±Î·Î», Ï€Î¿Î¹Î­Ï‰, Ï€Î¿Î½Î·ÏÏŒÏ‚, á¼Î½Î±...   \n",
       "\n",
       "                                             bigrams  \\\n",
       "0  [(Ï€ÏÎ¿ÏƒÏ„Î¯Î¸Î·Î¼Î¹, Ï…á¼±Î¿á½¶), (Ï…á¼±Î¿á½¶, Î¹ÏƒÏÎ±Î·Î»), (Î¹ÏƒÏÎ±Î·Î», ...   \n",
       "\n",
       "                                            trigrams  \\\n",
       "0  [(Ï€ÏÎ¿ÏƒÏ„Î¯Î¸Î·Î¼Î¹, Ï…á¼±Î¿á½¶, Î¹ÏƒÏÎ±Î·Î»), (Ï…á¼±Î¿á½¶, Î¹ÏƒÏÎ±Î·Î», Ï€Î¿...   \n",
       "\n",
       "                                                 pos  \\\n",
       "0  [coordinating_conjunction, verb, determiner, n...   \n",
       "\n",
       "                                              morpho  \\\n",
       "0  [[], [(habitual, imperfective, iterative, perf...   \n",
       "\n",
       "                                          vocabulary  \n",
       "0  [Î±á½Ï„ÏŒÏ‚, Î¹ÏƒÏÎ±Î·Î», ÎºÏÏÎ¹Î¿Ï‚, Ï€Î±ÏÎ±Î´Î¯Î´Ï‰Î¼Î¹, Ï€Î¿Î¹Î­Ï‰, Ï€Î¿Î½...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>verse</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemmata</th>\n",
       "      <th>tokens_filtered</th>\n",
       "      <th>lemmata_filtered</th>\n",
       "      <th>bigrams</th>\n",
       "      <th>trigrams</th>\n",
       "      <th>pos</th>\n",
       "      <th>morpho</th>\n",
       "      <th>vocabulary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ã€ŒPs 131:18ã€</td>\n",
       "      <td>Ï„Î¿á½ºÏ‚ á¼Ï‡Î¸ÏÎ¿á½ºÏ‚ Î±á½Ï„Î¿á¿¦ á¼Î½Î´ÏÏƒÏ‰ Î±á¼°ÏƒÏ‡ÏÎ½Î·Î½ á¼Ï€á½¶ Î´á½² Î±á½Ï„á½¸...</td>\n",
       "      <td>[Ï„Î¿á½ºÏ‚, á¼Ï‡Î¸ÏÎ¿á½ºÏ‚, Î±á½Ï„Î¿á¿¦, á¼Î½Î´ÏÏƒÏ‰, Î±á¼°ÏƒÏ‡ÏÎ½Î·Î½, á¼Ï€á½¶, ...</td>\n",
       "      <td>[á½, á¼Ï‡Î¸ÏÏŒÏ‚, Î±á½Ï„ÏŒÏ‚, á¼Î½Î´ÏÏ‰, Î±á¼°ÏƒÏ‡ÏÎ½Î·, á¼Ï€Î¯, Î´Î­, Î±á½...</td>\n",
       "      <td>[á¼Ï‡Î¸ÏÎ¿á½ºÏ‚, Î±á½Ï„Î¿á¿¦, á¼Î½Î´ÏÏƒÏ‰, Î±á¼°ÏƒÏ‡ÏÎ½Î·Î½, Î±á½Ï„á½¸Î½, á¼Î¾Î±Î½...</td>\n",
       "      <td>[á¼Ï‡Î¸ÏÏŒÏ‚, Î±á½Ï„ÏŒÏ‚, á¼Î½Î´ÏÏ‰, Î±á¼°ÏƒÏ‡ÏÎ½Î·, Î±á½Ï„ÏŒÏ‚, á¼Î¾Î±Î½Î¸Î­Ï‰...</td>\n",
       "      <td>[(á¼Ï‡Î¸ÏÏŒÏ‚, Î±á½Ï„ÏŒÏ‚), (Î±á½Ï„ÏŒÏ‚, á¼Î½Î´ÏÏ‰), (á¼Î½Î´ÏÏ‰, Î±á¼°ÏƒÏ‡...</td>\n",
       "      <td>[(á¼Ï‡Î¸ÏÏŒÏ‚, Î±á½Ï„ÏŒÏ‚, á¼Î½Î´ÏÏ‰), (Î±á½Ï„ÏŒÏ‚, á¼Î½Î´ÏÏ‰, Î±á¼°ÏƒÏ‡ÏÎ½...</td>\n",
       "      <td>[determiner, adjective, pronoun, verb, noun, a...</td>\n",
       "      <td>[[(nominative, accusative, ergative, absolutiv...</td>\n",
       "      <td>[Î±á¼°ÏƒÏ‡ÏÎ½Î·, Î±á½Ï„ÏŒÏ‚, á¼Î³Î¯Î±ÏƒÎ¼Î¬, á¼Î½Î´ÏÏ‰, á¼Î¾Î±Î½Î¸Î­Ï‰, á¼Ï‡Î¸ÏÏŒÏ‚]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         verse                                               text  \\\n",
       "0  ã€ŒPs 131:18ã€  Ï„Î¿á½ºÏ‚ á¼Ï‡Î¸ÏÎ¿á½ºÏ‚ Î±á½Ï„Î¿á¿¦ á¼Î½Î´ÏÏƒÏ‰ Î±á¼°ÏƒÏ‡ÏÎ½Î·Î½ á¼Ï€á½¶ Î´á½² Î±á½Ï„á½¸...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [Ï„Î¿á½ºÏ‚, á¼Ï‡Î¸ÏÎ¿á½ºÏ‚, Î±á½Ï„Î¿á¿¦, á¼Î½Î´ÏÏƒÏ‰, Î±á¼°ÏƒÏ‡ÏÎ½Î·Î½, á¼Ï€á½¶, ...   \n",
       "\n",
       "                                             lemmata  \\\n",
       "0  [á½, á¼Ï‡Î¸ÏÏŒÏ‚, Î±á½Ï„ÏŒÏ‚, á¼Î½Î´ÏÏ‰, Î±á¼°ÏƒÏ‡ÏÎ½Î·, á¼Ï€Î¯, Î´Î­, Î±á½...   \n",
       "\n",
       "                                     tokens_filtered  \\\n",
       "0  [á¼Ï‡Î¸ÏÎ¿á½ºÏ‚, Î±á½Ï„Î¿á¿¦, á¼Î½Î´ÏÏƒÏ‰, Î±á¼°ÏƒÏ‡ÏÎ½Î·Î½, Î±á½Ï„á½¸Î½, á¼Î¾Î±Î½...   \n",
       "\n",
       "                                    lemmata_filtered  \\\n",
       "0  [á¼Ï‡Î¸ÏÏŒÏ‚, Î±á½Ï„ÏŒÏ‚, á¼Î½Î´ÏÏ‰, Î±á¼°ÏƒÏ‡ÏÎ½Î·, Î±á½Ï„ÏŒÏ‚, á¼Î¾Î±Î½Î¸Î­Ï‰...   \n",
       "\n",
       "                                             bigrams  \\\n",
       "0  [(á¼Ï‡Î¸ÏÏŒÏ‚, Î±á½Ï„ÏŒÏ‚), (Î±á½Ï„ÏŒÏ‚, á¼Î½Î´ÏÏ‰), (á¼Î½Î´ÏÏ‰, Î±á¼°ÏƒÏ‡...   \n",
       "\n",
       "                                            trigrams  \\\n",
       "0  [(á¼Ï‡Î¸ÏÏŒÏ‚, Î±á½Ï„ÏŒÏ‚, á¼Î½Î´ÏÏ‰), (Î±á½Ï„ÏŒÏ‚, á¼Î½Î´ÏÏ‰, Î±á¼°ÏƒÏ‡ÏÎ½...   \n",
       "\n",
       "                                                 pos  \\\n",
       "0  [determiner, adjective, pronoun, verb, noun, a...   \n",
       "\n",
       "                                              morpho  \\\n",
       "0  [[(nominative, accusative, ergative, absolutiv...   \n",
       "\n",
       "                                          vocabulary  \n",
       "0  [Î±á¼°ÏƒÏ‡ÏÎ½Î·, Î±á½Ï„ÏŒÏ‚, á¼Î³Î¯Î±ÏƒÎ¼Î¬, á¼Î½Î´ÏÏ‰, á¼Î¾Î±Î½Î¸Î­Ï‰, á¼Ï‡Î¸ÏÏŒÏ‚]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- text  -----\n",
      "\n",
      "ÎºÎ±á½¶ Ï€ÏÎ¿ÏƒÎ­Î¸ÎµÎ½Ï„Î¿ Î¿á¼± Ï…á¼±Î¿á½¶ Î¹ÏƒÏÎ±Î·Î» Ï€Î¿Î¹á¿†ÏƒÎ±Î¹ Ï„á½¸ Ï€Î¿Î½Î·Ïá½¸Î½ á¼Î½Î±Î½Ï„Î¯Î¿Î½ ÎºÏ…ÏÎ¯Î¿Ï… ÎºÎ±á½¶ Ï€Î±ÏÎ­Î´Ï‰ÎºÎµÎ½ Î±á½Ï„Î¿á½ºÏ‚ ÎºÏÏÎ¹Î¿Ï‚ á¼Î½ Ï‡ÎµÎ¹Ïá½¶ á¼€Î»Î»Î¿Ï†ÏÎ»Ï‰Î½ Ï„ÎµÏƒÏƒÎ±ÏÎ¬ÎºÎ¿Î½Ï„Î± á¼”Ï„Î·\n",
      "Ï„Î¿á½ºÏ‚ á¼Ï‡Î¸ÏÎ¿á½ºÏ‚ Î±á½Ï„Î¿á¿¦ á¼Î½Î´ÏÏƒÏ‰ Î±á¼°ÏƒÏ‡ÏÎ½Î·Î½ á¼Ï€á½¶ Î´á½² Î±á½Ï„á½¸Î½ á¼Î¾Î±Î½Î¸Î®ÏƒÎµÎ¹ Ï„á½¸ á¼Î³Î¯Î±ÏƒÎ¼Î¬ Î¼Î¿Ï…\n",
      "Edit distance between 2 verses : 0.796875\n",
      "\n",
      "----- tokens  -----\n",
      "\n",
      "['ÎºÎ±á½¶', 'Ï€ÏÎ¿ÏƒÎ­Î¸ÎµÎ½Ï„Î¿', 'Î¿á¼±', 'Ï…á¼±Î¿á½¶', 'Î¹ÏƒÏÎ±Î·Î»', 'Ï€Î¿Î¹á¿†ÏƒÎ±Î¹', 'Ï„á½¸', 'Ï€Î¿Î½Î·Ïá½¸Î½', 'á¼Î½Î±Î½Ï„Î¯Î¿Î½', 'ÎºÏ…ÏÎ¯Î¿Ï…', 'ÎºÎ±á½¶', 'Ï€Î±ÏÎ­Î´Ï‰ÎºÎµÎ½', 'Î±á½Ï„Î¿á½ºÏ‚', 'ÎºÏÏÎ¹Î¿Ï‚', 'á¼Î½', 'Ï‡ÎµÎ¹Ïá½¶', 'á¼€Î»Î»Î¿Ï†ÏÎ»Ï‰Î½', 'Ï„ÎµÏƒÏƒÎ±ÏÎ¬ÎºÎ¿Î½Ï„Î±', 'á¼”Ï„Î·']\n",
      "['Ï„Î¿á½ºÏ‚', 'á¼Ï‡Î¸ÏÎ¿á½ºÏ‚', 'Î±á½Ï„Î¿á¿¦', 'á¼Î½Î´ÏÏƒÏ‰', 'Î±á¼°ÏƒÏ‡ÏÎ½Î·Î½', 'á¼Ï€á½¶', 'Î´á½²', 'Î±á½Ï„á½¸Î½', 'á¼Î¾Î±Î½Î¸Î®ÏƒÎµÎ¹', 'Ï„á½¸', 'á¼Î³Î¯Î±ÏƒÎ¼Î¬', 'Î¼Î¿Ï…']\n",
      "Edit distance between 2 verses : 1.0\n",
      "CPU times: user 745 Î¼s, sys: 0 ns, total: 745 Î¼s\n",
      "Wall time: 754 Î¼s\n",
      "\n",
      "----- lemmata  -----\n",
      "\n",
      "['ÎºÎ±Î¯', 'Ï€ÏÎ¿ÏƒÏ„Î¯Î¸Î·Î¼Î¹', 'á½', 'Ï…á¼±ÏŒÏ‚', 'Î¹ÏƒÏÎ±Î·Î»', 'Ï€Î¿Î¹Î­Ï‰', 'á½', 'Ï€Î¿Î½Î·ÏÏŒÏ‚', 'á¼Î½Î±Î½Ï„Î¯Î¿Î½', 'ÎºÏÏÎ¹Î¿Ï‚', 'ÎºÎ±Î¯', 'Ï€Î±ÏÎ±Î´Î¯Î´Ï‰Î¼Î¹', 'Î±á½Ï„ÏŒÏ‚', 'ÎºÏÏÎ¹Î¿Ï‚', 'á¼Î½', 'Ï‡ÎµÎ¯Ï', 'á¼€Î»Î»ÏŒÏ†Ï…Î»Î¿Ï‚', 'Ï„ÎµÏƒÏƒÎ±ÏÎ¬ÎºÎ¿Î½Ï„Î±', 'á¼”Ï„Î¿Ï‚']\n",
      "['á½', 'á¼Ï‡Î¸ÏÏŒÏ‚', 'Î±á½Ï„ÏŒÏ‚', 'á¼Î½Î´ÏÏ‰', 'Î±á¼°ÏƒÏ‡ÏÎ½Î·', 'á¼Ï€Î¯', 'Î´Î­', 'Î±á½Ï„ÏŒÏ‚', 'á¼Î¾Î±Î½Î¸Î­Ï‰', 'á½', 'á¼Î³Î¯Î±ÏƒÎ¼Î±', 'á¼Î³Ï']\n",
      "Edit distance between 2 verses : 0.8947368421052632\n",
      "\n",
      "----- tokens_filtered  -----\n",
      "\n",
      "['Ï€ÏÎ¿ÏƒÎ­Î¸ÎµÎ½Ï„Î¿', 'Ï…á¼±Î¿á½¶', 'Î¹ÏƒÏÎ±Î·Î»', 'Ï€Î¿Î¹á¿†ÏƒÎ±Î¹', 'Ï€Î¿Î½Î·Ïá½¸Î½', 'á¼Î½Î±Î½Ï„Î¯Î¿Î½', 'ÎºÏ…ÏÎ¯Î¿Ï…', 'Ï€Î±ÏÎ­Î´Ï‰ÎºÎµÎ½', 'Î±á½Ï„Î¿á½ºÏ‚', 'ÎºÏÏÎ¹Î¿Ï‚', 'Ï‡ÎµÎ¹Ïá½¶', 'á¼€Î»Î»Î¿Ï†ÏÎ»Ï‰Î½', 'Ï„ÎµÏƒÏƒÎ±ÏÎ¬ÎºÎ¿Î½Ï„Î±', 'á¼”Ï„Î·']\n",
      "['á¼Ï‡Î¸ÏÎ¿á½ºÏ‚', 'Î±á½Ï„Î¿á¿¦', 'á¼Î½Î´ÏÏƒÏ‰', 'Î±á¼°ÏƒÏ‡ÏÎ½Î·Î½', 'Î±á½Ï„á½¸Î½', 'á¼Î¾Î±Î½Î¸Î®ÏƒÎµÎ¹', 'á¼Î³Î¯Î±ÏƒÎ¼Î¬', 'Î¼Î¿Ï…']\n",
      "Edit distance between 2 verses : 1.0\n",
      "\n",
      "----- lemmata_filtered  -----\n",
      "\n",
      "['Ï€ÏÎ¿ÏƒÏ„Î¯Î¸Î·Î¼Î¹', 'Ï…á¼±Î¿á½¶', 'Î¹ÏƒÏÎ±Î·Î»', 'Ï€Î¿Î¹Î­Ï‰', 'Ï€Î¿Î½Î·ÏÏŒÏ‚', 'á¼Î½Î±Î½Ï„Î¯Î¿Î½', 'ÎºÏÏÎ¹Î¿Ï‚', 'Ï€Î±ÏÎ±Î´Î¯Î´Ï‰Î¼Î¹', 'Î±á½Ï„ÏŒÏ‚', 'ÎºÏÏÎ¹Î¿Ï‚', 'Ï‡ÎµÎ¯Ï', 'á¼€Î»Î»ÏŒÏ†Ï…Î»Î¿Ï‚', 'Ï„ÎµÏƒÏƒÎ±ÏÎ¬ÎºÎ¿Î½Ï„Î±', 'á¼”Ï„Î¿Ï‚']\n",
      "['á¼Ï‡Î¸ÏÏŒÏ‚', 'Î±á½Ï„ÏŒÏ‚', 'á¼Î½Î´ÏÏ‰', 'Î±á¼°ÏƒÏ‡ÏÎ½Î·', 'Î±á½Ï„ÏŒÏ‚', 'á¼Î¾Î±Î½Î¸Î­Ï‰', 'á¼Î³Î¯Î±ÏƒÎ¼Î¬']\n",
      "Edit distance between 2 verses : 0.9285714285714286\n",
      "\n",
      "----- pos  -----\n",
      "\n",
      "['coordinating_conjunction', 'verb', 'determiner', 'noun', 'adjective', 'verb', 'determiner', 'adjective', 'adposition', 'noun', 'coordinating_conjunction', 'verb', 'pronoun', 'noun', 'adposition', 'noun', 'verb', 'numeral', 'noun']\n",
      "['determiner', 'adjective', 'pronoun', 'verb', 'noun', 'adposition', 'adverb', 'pronoun', 'verb', 'determiner', 'noun', 'pronoun']\n",
      "Edit distance between 2 verses : 0.7368421052631579\n",
      "\n",
      "----- morpho  -----\n",
      "\n",
      "[{}, {Aspect: [perfective], Mood: [indicative], Number: [plural], Person: [third], Tense: [past], VerbForm: [finite], Voice: [middle]}, {Case: [nominative], Definiteness: [definite], Gender: [masculine], Number: [plural], PronominalType: [demonstrative]}, {Case: [nominative], Gender: [masculine], Number: [plural]}, {Case: [nominative], Degree: [positive], Gender: [masculine], Number: [plural]}, {Aspect: [perfective], Tense: [past], VerbForm: [infinitive], Voice: [active]}, {Case: [accusative], Definiteness: [definite], Gender: [neuter], Number: [singular], PronominalType: [demonstrative]}, {Case: [nominative], Degree: [positive], Gender: [neuter], Number: [singular]}, {}, {Case: [genitive], Gender: [masculine], Number: [singular]}, {}, {Aspect: [perfective], Mood: [indicative], Number: [singular], Person: [third], Tense: [past], VerbForm: [finite], Voice: [active]}, {Case: [accusative], Gender: [masculine], Number: [plural], Person: [third], PronominalType: [personal]}, {Case: [nominative], Gender: [masculine], Number: [singular]}, {}, {Case: [dative], Gender: [feminine], Number: [singular]}, {Case: [nominative], Gender: [masculine], Number: [singular], Tense: [present], VerbForm: [participle], Voice: [active]}, {}, {Case: [accusative], Gender: [neuter], Number: [plural]}]\n",
      "[{Case: [accusative], Definiteness: [definite], Gender: [masculine], Number: [plural], PronominalType: [demonstrative]}, {Case: [accusative], Degree: [positive], Gender: [masculine], Number: [plural]}, {Case: [genitive], Gender: [masculine], Number: [singular], Person: [third], PronominalType: [personal]}, {Mood: [indicative], Number: [singular], Person: [first], Tense: [future], VerbForm: [finite], Voice: [active]}, {Case: [accusative], Gender: [feminine], Number: [singular]}, {}, {}, {Case: [accusative], Gender: [masculine], Number: [singular], Person: [third], PronominalType: [personal]}, {Mood: [indicative], Number: [singular], Person: [third], Tense: [future], VerbForm: [finite], Voice: [active]}, {Case: [accusative], Definiteness: [definite], Gender: [neuter], Number: [singular], PronominalType: [demonstrative]}, {Case: [accusative], Gender: [neuter], Number: [singular]}, {Case: [genitive], Gender: [masculine], Number: [singular], Person: [first], PronominalType: [personal]}]\n",
      "Edit distance between 2 verses : 1.0\n",
      "\n",
      "----- vocabulary  -----\n",
      "\n",
      "['Î±á½Ï„ÏŒÏ‚', 'Î¹ÏƒÏÎ±Î·Î»', 'ÎºÏÏÎ¹Î¿Ï‚', 'Ï€Î±ÏÎ±Î´Î¯Î´Ï‰Î¼Î¹', 'Ï€Î¿Î¹Î­Ï‰', 'Ï€Î¿Î½Î·ÏÏŒÏ‚', 'Ï€ÏÎ¿ÏƒÏ„Î¯Î¸Î·Î¼Î¹', 'Ï„ÎµÏƒÏƒÎ±ÏÎ¬ÎºÎ¿Î½Ï„Î±', 'Ï…á¼±Î¿á½¶', 'Ï‡ÎµÎ¯Ï', 'á¼€Î»Î»ÏŒÏ†Ï…Î»Î¿Ï‚', 'á¼Î½Î±Î½Ï„Î¯Î¿Î½', 'á¼”Ï„Î¿Ï‚']\n",
      "['Î±á¼°ÏƒÏ‡ÏÎ½Î·', 'Î±á½Ï„ÏŒÏ‚', 'á¼Î³Î¯Î±ÏƒÎ¼Î¬', 'á¼Î½Î´ÏÏ‰', 'á¼Î¾Î±Î½Î¸Î­Ï‰', 'á¼Ï‡Î¸ÏÏŒÏ‚']\n",
      "Edit distance between 2 verses : 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Edit distance test on verses\n",
    "id_verse_1 = \"ã€ŒJudgA 13:1ã€\"\n",
    "# id_verse_2 = \"Matt 16:21\"\n",
    "id_verse_2 = \"ã€ŒPs 131:18ã€\"\n",
    "\n",
    "def extract_verse(id_verse,df):   \n",
    "    extracted_verse = df[df.verse == id_verse].reset_index(drop=True)\n",
    "    display(extracted_verse)\n",
    "    return extracted_verse\n",
    "\n",
    "# Find the verses in dataframes\n",
    "v_1 = extract_verse(id_verse_1,NT)\n",
    "v_2 = extract_verse(id_verse_2,NT)\n",
    "\n",
    "def compute_distance(v_1,v_2,method,info=False,transpos=False):\n",
    "    str_v1 = v_1[method][0]\n",
    "    len_v1 = len(str_v1)\n",
    "    str_v2 = v_2[method][0]\n",
    "    len_v2 = len(str_v2)\n",
    "\n",
    "    transpos = False\n",
    "    # Compute distance\n",
    "    # edit_dist = (nl.edit_distance(str_v1, str_v2,substitution_cost=1,transpositions=transpos)-abs(len_v1-len_v2))/min(len_v1,len_v2)  # Normalized distance 1\n",
    "    # edit_dist = jaro_similarity(str_v1, str_v2)  # Normalized distance 1\n",
    "    # edit_dist = 1-(pylcs.lcs_sequence_length(str_v1,str_v2)-abs(len_v1-len_v2))/min(len_v1,len_v2)\n",
    "    edit_dist = (nl.edit_distance(str_v1, str_v2,substitution_cost=1,transpositions=transpos))/np.max([len_v1,len_v2])  # Normalized distance  2  \n",
    "    # edit_dist = (nl.edit_distance(str_v1, str_v2)-abs(len_v1-len_v2))/min(len_v1,len_v2)  # Raw distance (unity : words/characters)\n",
    "\n",
    "    if info == True:\n",
    "        print(\"\\n-----\",method,\" -----\\n\")\n",
    "        print(str_v1)\n",
    "        print(str_v2)\n",
    "        print(f\"Edit distance between 2 verses :\",edit_dist)\n",
    "    return edit_dist\n",
    "\n",
    "compute_distance(v_1,v_2,'text',True)\n",
    "%time compute_distance(v_1,v_2,'tokens',True,True)\n",
    "compute_distance(v_1,v_2,'lemmata',True,True)\n",
    "compute_distance(v_1,v_2,'tokens_filtered',True,True)\n",
    "compute_distance(v_1,v_2,'lemmata_filtered',True,True)\n",
    "compute_distance(v_1,v_2,'pos',True,True)\n",
    "compute_distance(v_1,v_2,'morpho',True,True)\n",
    "compute_distance(v_1,v_2,'vocabulary',True,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "efb3737f-c278-41e6-b70b-7e76902a561a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>verse</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemmata</th>\n",
       "      <th>tokens_filtered</th>\n",
       "      <th>lemmata_filtered</th>\n",
       "      <th>bigrams</th>\n",
       "      <th>trigrams</th>\n",
       "      <th>pos</th>\n",
       "      <th>morpho</th>\n",
       "      <th>vocabulary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [verse, text, tokens, lemmata, tokens_filtered, lemmata_filtered, bigrams, trigrams, pos, morpho, vocabulary]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/range.py:413\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 413\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_range\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[0;31mValueError\u001b[0m: 0 is not in range",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m id_verse_1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMark 1:1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m v_1 \u001b[38;5;241m=\u001b[39m extract_verse(id_verse_1,Evangiles)\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mv_1\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvocabulary\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py:1121\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m-> 1121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py:1237\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[1;32m   1236\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1237\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[1;32m   1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/range.py:415\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_range\u001b[38;5;241m.\u001b[39mindex(new_key)\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 415\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Hashable):\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "id_verse_1 = \"Mark 1:1\"\n",
    "\n",
    "v_1 = extract_verse(id_verse_1,Evangiles)\n",
    "print(v_1['vocabulary'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd43c7f-1e88-4ce1-8c54-a9eee28d8991",
   "metadata": {},
   "source": [
    "# Save dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd365db1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>verse</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemmata</th>\n",
       "      <th>tokens_filtered</th>\n",
       "      <th>lemmata_filtered</th>\n",
       "      <th>bigrams</th>\n",
       "      <th>trigrams</th>\n",
       "      <th>pos</th>\n",
       "      <th>morpho</th>\n",
       "      <th>vocabulary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1709</th>\n",
       "      <td>ã€ŒPs 103:15ã€</td>\n",
       "      <td>ÎºÎ±á½¶ Î¿á¼¶Î½Î¿Ï‚ Îµá½Ï†ÏÎ±Î¯Î½ÎµÎ¹ ÎºÎ±ÏÎ´Î¯Î±Î½ á¼€Î½Î¸ÏÏÏ€Î¿Ï… Ï„Î¿á¿¦ á¼±Î»Î±Ïá¿¦...</td>\n",
       "      <td>[ÎºÎ±á½¶, Î¿á¼¶Î½Î¿Ï‚, Îµá½Ï†ÏÎ±Î¯Î½ÎµÎ¹, ÎºÎ±ÏÎ´Î¯Î±Î½, á¼€Î½Î¸ÏÏÏ€Î¿Ï…, Ï„Î¿á¿¦...</td>\n",
       "      <td>[ÎºÎ±Î¯, Î¿á¼¶Î½Î¿Ï‚, Îµá½Ï†ÏÎ±Î¯Î½Ï‰, ÎºÎ±ÏÎ´Î¯Î±, á¼„Î½Î¸ÏÏ‰Ï€Î¿Ï‚, á½, á¼±Î»...</td>\n",
       "      <td>[Î¿á¼¶Î½Î¿Ï‚, Îµá½Ï†ÏÎ±Î¯Î½ÎµÎ¹, ÎºÎ±ÏÎ´Î¯Î±Î½, á¼€Î½Î¸ÏÏÏ€Î¿Ï…, á¼±Î»Î±Ïá¿¦Î½Î±Î¹...</td>\n",
       "      <td>[Î¿á¼¶Î½Î¿Ï‚, ÎµÏ…Ì“Ï†ÏÎ±Î¯Î½Ï‰, ÎºÎ±ÏÎ´Î¯Î±, á¼„Î½Î¸ÏÏ‰Ï€Î¿Ï‚, á¼±Î»Î±ÏÏÎ½Ï‰, ...</td>\n",
       "      <td>[(Î¿á¼¶Î½Î¿Ï‚, ÎµÏ…Ì“Ï†ÏÎ±Î¯Î½Ï‰), (ÎµÏ…Ì“Ï†ÏÎ±Î¯Î½Ï‰, ÎºÎ±ÏÎ´Î¯Î±), (ÎºÎ±Ï...</td>\n",
       "      <td>[(Î¿á¼¶Î½Î¿Ï‚, ÎµÏ…Ì“Ï†ÏÎ±Î¯Î½Ï‰, ÎºÎ±ÏÎ´Î¯Î±), (ÎµÏ…Ì“Ï†ÏÎ±Î¯Î½Ï‰, ÎºÎ±ÏÎ´Î¯...</td>\n",
       "      <td>[coordinating_conjunction, noun, verb, noun, n...</td>\n",
       "      <td>[[], [(nominative, accusative, ergative, absol...</td>\n",
       "      <td>[ÎµÏ…Ì“Ï†ÏÎ±Î¯Î½Ï‰, ÎºÎ±ÏÎ´Î¯Î±, Î¿á¼¶Î½Î¿Ï‚, Ï€ÏÏŒÏƒÏ‰Ï€Î¿Î½, ÏƒÏ„Î·ÏÎ¯Î¶Ï‰, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2445</th>\n",
       "      <td>ã€ŒPs 143:15ã€</td>\n",
       "      <td>á¼Î¼Î±ÎºÎ¬ÏÎ¹ÏƒÎ±Î½ Ï„á½¸Î½ Î»Î±ÏŒÎ½ á¾§ Ï„Î±á¿¦Ï„Î¬ á¼ÏƒÏ„Î¹Î½ Î¼Î±ÎºÎ¬ÏÎ¹Î¿Ï‚ á½ Î»...</td>\n",
       "      <td>[á¼Î¼Î±ÎºÎ¬ÏÎ¹ÏƒÎ±Î½, Ï„á½¸Î½, Î»Î±ÏŒÎ½, á¾§, Ï„Î±á¿¦Ï„Î¬, á¼ÏƒÏ„Î¹Î½, Î¼Î±ÎºÎ¬Ï...</td>\n",
       "      <td>[á¼Î¼Î±ÎºÎ¬ÏÎ¹ÏƒÎ±Î½, á½, Î»Î±ÏŒÏ‚, á½…Ï‚, Î¿á½—Ï„Î¿Ï‚, Îµá¼°Î¼Î¯, Î¼Î±ÎºÎ¬ÏÎ¹Î¿...</td>\n",
       "      <td>[á¼Î¼Î±ÎºÎ¬ÏÎ¹ÏƒÎ±Î½, Î»Î±ÏŒÎ½, á¼ÏƒÏ„Î¹Î½, Î¼Î±ÎºÎ¬ÏÎ¹Î¿Ï‚, Î»Î±ÏŒÏ‚, ÎºÏÏÎ¹...</td>\n",
       "      <td>[Î¼Î±ÎºÎ±ÏÎ¯Î¶Ï‰, Î»Î±ÏŒÏ‚, Îµá¼°Î¼Î¯, Î¼Î±ÎºÎ¬ÏÎ¹Î¿Ï‚, Î»Î±ÏŒÏ‚, ÎºÏÏÎ¹Î¿Ï‚,...</td>\n",
       "      <td>[(Î¼Î±ÎºÎ±ÏÎ¯Î¶Ï‰, Î»Î±ÏŒÏ‚), (Î»Î±ÏŒÏ‚, Îµá¼°Î¼Î¯), (Îµá¼°Î¼Î¯, Î¼Î±ÎºÎ¬ÏÎ¹...</td>\n",
       "      <td>[(Î¼Î±ÎºÎ±ÏÎ¯Î¶Ï‰, Î»Î±ÏŒÏ‚, Îµá¼°Î¼Î¯), (Î»Î±ÏŒÏ‚, Îµá¼°Î¼Î¯, Î¼Î±ÎºÎ¬ÏÎ¹Î¿Ï‚...</td>\n",
       "      <td>[verb, determiner, noun, pronoun, adjective, a...</td>\n",
       "      <td>[[(habitual, imperfective, iterative, perfecti...</td>\n",
       "      <td>[Î±á½Ï„ÏŒÏ‚, Îµá¼°Î¼Î¯, Î¸ÎµÏŒÏ‚, ÎºÏÏÎ¹Î¿Ï‚, Î»Î±ÏŒÏ‚, Î¼Î±ÎºÎ¬ÏÎ¹Î¿Ï‚, Î¼Î±...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1588</th>\n",
       "      <td>ã€ŒPs 95:1ã€</td>\n",
       "      <td>á½…Ï„Îµ á½ Î¿á¼¶ÎºÎ¿Ï‚ á¾ ÎºÎ¿Î´Î¿Î¼Îµá¿–Ï„Î¿ Î¼ÎµÏ„á½° Ï„á½´Î½ Î±á¼°Ï‡Î¼Î±Î»Ï‰ÏƒÎ¯Î±Î½ á¾ Î´...</td>\n",
       "      <td>[á½…Ï„Îµ, á½, Î¿á¼¶ÎºÎ¿Ï‚, á¾ ÎºÎ¿Î´Î¿Î¼Îµá¿–Ï„Î¿, Î¼ÎµÏ„á½°, Ï„á½´Î½, Î±á¼°Ï‡Î¼Î±Î»Ï‰...</td>\n",
       "      <td>[á½…Ï„Îµ, á½, Î¿á¼¶ÎºÎ¿Ï‚, á¾ ÎºÎ¿Î´Î¿Î¼Î­Î¿Î¼Î±Î¹, Î¼ÎµÏ„Î¬, á½, Î±á¼°Ï‡Î¼Î±Î»Ï‰Ïƒ...</td>\n",
       "      <td>[á½…Ï„Îµ, Î¿á¼¶ÎºÎ¿Ï‚, á¾ ÎºÎ¿Î´Î¿Î¼Îµá¿–Ï„Î¿, Î±á¼°Ï‡Î¼Î±Î»Ï‰ÏƒÎ¯Î±Î½, á¾ Î´á½´, Î´Î±Ï…...</td>\n",
       "      <td>[Î¿á¼¶ÎºÎ¿Ï‚, Î¿á¼°ÎºÎ¿Î´Î¿Î¼Î­Ï‰, Î±á¼°Ï‡Î¼Î±Î»Ï‰ÏƒÎ¯Î±, á¾ Î´á½´, Î´Î±Ï…Î¹Î´, á¼€ÎµÎ¯...</td>\n",
       "      <td>[(Î¿á¼¶ÎºÎ¿Ï‚, Î¿á¼°ÎºÎ¿Î´Î¿Î¼Î­Ï‰), (Î¿á¼°ÎºÎ¿Î´Î¿Î¼Î­Ï‰, Î±á¼°Ï‡Î¼Î±Î»Ï‰ÏƒÎ¯Î±), ...</td>\n",
       "      <td>[(Î¿á¼¶ÎºÎ¿Ï‚, Î¿á¼°ÎºÎ¿Î´Î¿Î¼Î­Ï‰, Î±á¼°Ï‡Î¼Î±Î»Ï‰ÏƒÎ¯Î±), (Î¿á¼°ÎºÎ¿Î´Î¿Î¼Î­Ï‰, Î±...</td>\n",
       "      <td>[subordinating_conjunction, determiner, noun, ...</td>\n",
       "      <td>[[], [(nominative, accusative, ergative, absol...</td>\n",
       "      <td>[Î±á¼°Ï‡Î¼Î±Î»Ï‰ÏƒÎ¯Î±, Î³á¿†, Î´Î±Ï…Î¹Î´, ÎºÎ±Î¹Î½ÏŒÏ‚, ÎºÏÏÎ¹Î¿Ï‚, Î¿á¼°ÎºÎ¿Î´Î¿...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>ã€ŒPs 27:9ã€</td>\n",
       "      <td>Ïƒá¿¶ÏƒÎ¿Î½ Ï„á½¸Î½ Î»Î±ÏŒÎ½ ÏƒÎ¿Ï… ÎºÎ±á½¶ Îµá½Î»ÏŒÎ³Î·ÏƒÎ¿Î½ Ï„á½´Î½ ÎºÎ»Î·ÏÎ¿Î½Î¿Î¼Î¯...</td>\n",
       "      <td>[Ïƒá¿¶ÏƒÎ¿Î½, Ï„á½¸Î½, Î»Î±ÏŒÎ½, ÏƒÎ¿Ï…, ÎºÎ±á½¶, Îµá½Î»ÏŒÎ³Î·ÏƒÎ¿Î½, Ï„á½´Î½, Îº...</td>\n",
       "      <td>[Ïƒá¿´Î¶Ï‰, á½, Î»Î±ÏŒÏ‚, ÏƒÏ, ÎºÎ±Î¯, Îµá½Î»ÏŒÎ³Î·ÏƒÎ¿Î½, á½, ÎºÎ»Î·ÏÎ¿Î½Î¿...</td>\n",
       "      <td>[Ïƒá¿¶ÏƒÎ¿Î½, Î»Î±ÏŒÎ½, ÏƒÎ¿Ï…, Îµá½Î»ÏŒÎ³Î·ÏƒÎ¿Î½, ÎºÎ»Î·ÏÎ¿Î½Î¿Î¼Î¯Î±Î½, ÏƒÎ¿Ï…...</td>\n",
       "      <td>[ÏƒÏÎ¶Ï‰, Î»Î±ÏŒÏ‚, ÎµÏ…Ì“Î»Î¿Î³Î­Ï‰, ÎºÎ»Î·ÏÎ¿Î½Î¿Î¼Î¯Î±, Ï€Î¿Î¯Î¼Î±Î½Î¿Î½, Î±...</td>\n",
       "      <td>[(ÏƒÏÎ¶Ï‰, Î»Î±ÏŒÏ‚), (Î»Î±ÏŒÏ‚, ÎµÏ…Ì“Î»Î¿Î³Î­Ï‰), (ÎµÏ…Ì“Î»Î¿Î³Î­Ï‰, ÎºÎ»...</td>\n",
       "      <td>[(ÏƒÏÎ¶Ï‰, Î»Î±ÏŒÏ‚, ÎµÏ…Ì“Î»Î¿Î³Î­Ï‰), (Î»Î±ÏŒÏ‚, ÎµÏ…Ì“Î»Î¿Î³Î­Ï‰, ÎºÎ»Î·Ï...</td>\n",
       "      <td>[verb, determiner, noun, pronoun, coordinating...</td>\n",
       "      <td>[[(habitual, imperfective, iterative, perfecti...</td>\n",
       "      <td>[Î±á¼°ÏÎ½, Î±á½Ï„ÏŒÏ‚, ÎµÏ…Ì“Î»Î¿Î³Î­Ï‰, ÎºÎ»Î·ÏÎ¿Î½Î¿Î¼Î¯Î±, Î»Î±ÏŒÏ‚, Ï€Î¿Î¯Î¼...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1791</th>\n",
       "      <td>ã€ŒPs 105:17ã€</td>\n",
       "      <td>á¼ Î½Î¿Î¯Ï‡Î¸Î· á¼¡ Î³á¿† ÎºÎ±á½¶ ÎºÎ±Ï„Î­Ï€Î¹ÎµÎ½ Î´Î±Î¸Î±Î½ ÎºÎ±á½¶ á¼ÎºÎ¬Î»Ï…ÏˆÎµÎ½ á¼...</td>\n",
       "      <td>[á¼ Î½Î¿Î¯Ï‡Î¸Î·, á¼¡, Î³á¿†, ÎºÎ±á½¶, ÎºÎ±Ï„Î­Ï€Î¹ÎµÎ½, Î´Î±Î¸Î±Î½, ÎºÎ±á½¶, á¼Îº...</td>\n",
       "      <td>[á¼€Î½Î¿Î¯Î³Î½Ï…Î¼Î¹, á½, Î³á¿†, ÎºÎ±Î¯, ÎºÎ±Ï„Î±Ï€Î¯Î½Ï‰, Î´Î±Î¸Î±Î½, ÎºÎ±Î¯, ...</td>\n",
       "      <td>[á¼ Î½Î¿Î¯Ï‡Î¸Î·, Î³á¿†, ÎºÎ±Ï„Î­Ï€Î¹ÎµÎ½, Î´Î±Î¸Î±Î½, á¼ÎºÎ¬Î»Ï…ÏˆÎµÎ½, ÏƒÏ…Î½Î±Î³...</td>\n",
       "      <td>[á¼€Î½Î¿Î¯Î³Î½Ï…Î¼Î¹, Î³á¿†, ÎºÎ±Ï„Î±Ï€Î¯Î½Ï‰, Î´Î±Î¸Î±Î½, ÎºÎ±Î»ÏÏ€Ï„Ï‰, ÏƒÏ…Î½Î±...</td>\n",
       "      <td>[(á¼€Î½Î¿Î¯Î³Î½Ï…Î¼Î¹, Î³á¿†), (Î³á¿†, ÎºÎ±Ï„Î±Ï€Î¯Î½Ï‰), (ÎºÎ±Ï„Î±Ï€Î¯Î½Ï‰, Î´...</td>\n",
       "      <td>[(á¼€Î½Î¿Î¯Î³Î½Ï…Î¼Î¹, Î³á¿†, ÎºÎ±Ï„Î±Ï€Î¯Î½Ï‰), (Î³á¿†, ÎºÎ±Ï„Î±Ï€Î¯Î½Ï‰, Î´Î±Î¸...</td>\n",
       "      <td>[verb, determiner, noun, coordinating_conjunct...</td>\n",
       "      <td>[[(habitual, imperfective, iterative, perfecti...</td>\n",
       "      <td>[Î±Î²Î¹ÏÏ‰Î½, Î³á¿†, Î´Î±Î¸Î±Î½, ÎºÎ±Î»ÏÏ€Ï„Ï‰, ÎºÎ±Ï„Î±Ï€Î¯Î½Ï‰, ÏƒÏ…Î½Î±Î³Ï‰Î³...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            verse                                               text  \\\n",
       "1709  ã€ŒPs 103:15ã€  ÎºÎ±á½¶ Î¿á¼¶Î½Î¿Ï‚ Îµá½Ï†ÏÎ±Î¯Î½ÎµÎ¹ ÎºÎ±ÏÎ´Î¯Î±Î½ á¼€Î½Î¸ÏÏÏ€Î¿Ï… Ï„Î¿á¿¦ á¼±Î»Î±Ïá¿¦...   \n",
       "2445  ã€ŒPs 143:15ã€  á¼Î¼Î±ÎºÎ¬ÏÎ¹ÏƒÎ±Î½ Ï„á½¸Î½ Î»Î±ÏŒÎ½ á¾§ Ï„Î±á¿¦Ï„Î¬ á¼ÏƒÏ„Î¹Î½ Î¼Î±ÎºÎ¬ÏÎ¹Î¿Ï‚ á½ Î»...   \n",
       "1588    ã€ŒPs 95:1ã€  á½…Ï„Îµ á½ Î¿á¼¶ÎºÎ¿Ï‚ á¾ ÎºÎ¿Î´Î¿Î¼Îµá¿–Ï„Î¿ Î¼ÎµÏ„á½° Ï„á½´Î½ Î±á¼°Ï‡Î¼Î±Î»Ï‰ÏƒÎ¯Î±Î½ á¾ Î´...   \n",
       "381     ã€ŒPs 27:9ã€  Ïƒá¿¶ÏƒÎ¿Î½ Ï„á½¸Î½ Î»Î±ÏŒÎ½ ÏƒÎ¿Ï… ÎºÎ±á½¶ Îµá½Î»ÏŒÎ³Î·ÏƒÎ¿Î½ Ï„á½´Î½ ÎºÎ»Î·ÏÎ¿Î½Î¿Î¼Î¯...   \n",
       "1791  ã€ŒPs 105:17ã€  á¼ Î½Î¿Î¯Ï‡Î¸Î· á¼¡ Î³á¿† ÎºÎ±á½¶ ÎºÎ±Ï„Î­Ï€Î¹ÎµÎ½ Î´Î±Î¸Î±Î½ ÎºÎ±á½¶ á¼ÎºÎ¬Î»Ï…ÏˆÎµÎ½ á¼...   \n",
       "\n",
       "                                                 tokens  \\\n",
       "1709  [ÎºÎ±á½¶, Î¿á¼¶Î½Î¿Ï‚, Îµá½Ï†ÏÎ±Î¯Î½ÎµÎ¹, ÎºÎ±ÏÎ´Î¯Î±Î½, á¼€Î½Î¸ÏÏÏ€Î¿Ï…, Ï„Î¿á¿¦...   \n",
       "2445  [á¼Î¼Î±ÎºÎ¬ÏÎ¹ÏƒÎ±Î½, Ï„á½¸Î½, Î»Î±ÏŒÎ½, á¾§, Ï„Î±á¿¦Ï„Î¬, á¼ÏƒÏ„Î¹Î½, Î¼Î±ÎºÎ¬Ï...   \n",
       "1588  [á½…Ï„Îµ, á½, Î¿á¼¶ÎºÎ¿Ï‚, á¾ ÎºÎ¿Î´Î¿Î¼Îµá¿–Ï„Î¿, Î¼ÎµÏ„á½°, Ï„á½´Î½, Î±á¼°Ï‡Î¼Î±Î»Ï‰...   \n",
       "381   [Ïƒá¿¶ÏƒÎ¿Î½, Ï„á½¸Î½, Î»Î±ÏŒÎ½, ÏƒÎ¿Ï…, ÎºÎ±á½¶, Îµá½Î»ÏŒÎ³Î·ÏƒÎ¿Î½, Ï„á½´Î½, Îº...   \n",
       "1791  [á¼ Î½Î¿Î¯Ï‡Î¸Î·, á¼¡, Î³á¿†, ÎºÎ±á½¶, ÎºÎ±Ï„Î­Ï€Î¹ÎµÎ½, Î´Î±Î¸Î±Î½, ÎºÎ±á½¶, á¼Îº...   \n",
       "\n",
       "                                                lemmata  \\\n",
       "1709  [ÎºÎ±Î¯, Î¿á¼¶Î½Î¿Ï‚, Îµá½Ï†ÏÎ±Î¯Î½Ï‰, ÎºÎ±ÏÎ´Î¯Î±, á¼„Î½Î¸ÏÏ‰Ï€Î¿Ï‚, á½, á¼±Î»...   \n",
       "2445  [á¼Î¼Î±ÎºÎ¬ÏÎ¹ÏƒÎ±Î½, á½, Î»Î±ÏŒÏ‚, á½…Ï‚, Î¿á½—Ï„Î¿Ï‚, Îµá¼°Î¼Î¯, Î¼Î±ÎºÎ¬ÏÎ¹Î¿...   \n",
       "1588  [á½…Ï„Îµ, á½, Î¿á¼¶ÎºÎ¿Ï‚, á¾ ÎºÎ¿Î´Î¿Î¼Î­Î¿Î¼Î±Î¹, Î¼ÎµÏ„Î¬, á½, Î±á¼°Ï‡Î¼Î±Î»Ï‰Ïƒ...   \n",
       "381   [Ïƒá¿´Î¶Ï‰, á½, Î»Î±ÏŒÏ‚, ÏƒÏ, ÎºÎ±Î¯, Îµá½Î»ÏŒÎ³Î·ÏƒÎ¿Î½, á½, ÎºÎ»Î·ÏÎ¿Î½Î¿...   \n",
       "1791  [á¼€Î½Î¿Î¯Î³Î½Ï…Î¼Î¹, á½, Î³á¿†, ÎºÎ±Î¯, ÎºÎ±Ï„Î±Ï€Î¯Î½Ï‰, Î´Î±Î¸Î±Î½, ÎºÎ±Î¯, ...   \n",
       "\n",
       "                                        tokens_filtered  \\\n",
       "1709  [Î¿á¼¶Î½Î¿Ï‚, Îµá½Ï†ÏÎ±Î¯Î½ÎµÎ¹, ÎºÎ±ÏÎ´Î¯Î±Î½, á¼€Î½Î¸ÏÏÏ€Î¿Ï…, á¼±Î»Î±Ïá¿¦Î½Î±Î¹...   \n",
       "2445  [á¼Î¼Î±ÎºÎ¬ÏÎ¹ÏƒÎ±Î½, Î»Î±ÏŒÎ½, á¼ÏƒÏ„Î¹Î½, Î¼Î±ÎºÎ¬ÏÎ¹Î¿Ï‚, Î»Î±ÏŒÏ‚, ÎºÏÏÎ¹...   \n",
       "1588  [á½…Ï„Îµ, Î¿á¼¶ÎºÎ¿Ï‚, á¾ ÎºÎ¿Î´Î¿Î¼Îµá¿–Ï„Î¿, Î±á¼°Ï‡Î¼Î±Î»Ï‰ÏƒÎ¯Î±Î½, á¾ Î´á½´, Î´Î±Ï…...   \n",
       "381   [Ïƒá¿¶ÏƒÎ¿Î½, Î»Î±ÏŒÎ½, ÏƒÎ¿Ï…, Îµá½Î»ÏŒÎ³Î·ÏƒÎ¿Î½, ÎºÎ»Î·ÏÎ¿Î½Î¿Î¼Î¯Î±Î½, ÏƒÎ¿Ï…...   \n",
       "1791  [á¼ Î½Î¿Î¯Ï‡Î¸Î·, Î³á¿†, ÎºÎ±Ï„Î­Ï€Î¹ÎµÎ½, Î´Î±Î¸Î±Î½, á¼ÎºÎ¬Î»Ï…ÏˆÎµÎ½, ÏƒÏ…Î½Î±Î³...   \n",
       "\n",
       "                                       lemmata_filtered  \\\n",
       "1709  [Î¿á¼¶Î½Î¿Ï‚, ÎµÏ…Ì“Ï†ÏÎ±Î¯Î½Ï‰, ÎºÎ±ÏÎ´Î¯Î±, á¼„Î½Î¸ÏÏ‰Ï€Î¿Ï‚, á¼±Î»Î±ÏÏÎ½Ï‰, ...   \n",
       "2445  [Î¼Î±ÎºÎ±ÏÎ¯Î¶Ï‰, Î»Î±ÏŒÏ‚, Îµá¼°Î¼Î¯, Î¼Î±ÎºÎ¬ÏÎ¹Î¿Ï‚, Î»Î±ÏŒÏ‚, ÎºÏÏÎ¹Î¿Ï‚,...   \n",
       "1588  [Î¿á¼¶ÎºÎ¿Ï‚, Î¿á¼°ÎºÎ¿Î´Î¿Î¼Î­Ï‰, Î±á¼°Ï‡Î¼Î±Î»Ï‰ÏƒÎ¯Î±, á¾ Î´á½´, Î´Î±Ï…Î¹Î´, á¼€ÎµÎ¯...   \n",
       "381   [ÏƒÏÎ¶Ï‰, Î»Î±ÏŒÏ‚, ÎµÏ…Ì“Î»Î¿Î³Î­Ï‰, ÎºÎ»Î·ÏÎ¿Î½Î¿Î¼Î¯Î±, Ï€Î¿Î¯Î¼Î±Î½Î¿Î½, Î±...   \n",
       "1791  [á¼€Î½Î¿Î¯Î³Î½Ï…Î¼Î¹, Î³á¿†, ÎºÎ±Ï„Î±Ï€Î¯Î½Ï‰, Î´Î±Î¸Î±Î½, ÎºÎ±Î»ÏÏ€Ï„Ï‰, ÏƒÏ…Î½Î±...   \n",
       "\n",
       "                                                bigrams  \\\n",
       "1709  [(Î¿á¼¶Î½Î¿Ï‚, ÎµÏ…Ì“Ï†ÏÎ±Î¯Î½Ï‰), (ÎµÏ…Ì“Ï†ÏÎ±Î¯Î½Ï‰, ÎºÎ±ÏÎ´Î¯Î±), (ÎºÎ±Ï...   \n",
       "2445  [(Î¼Î±ÎºÎ±ÏÎ¯Î¶Ï‰, Î»Î±ÏŒÏ‚), (Î»Î±ÏŒÏ‚, Îµá¼°Î¼Î¯), (Îµá¼°Î¼Î¯, Î¼Î±ÎºÎ¬ÏÎ¹...   \n",
       "1588  [(Î¿á¼¶ÎºÎ¿Ï‚, Î¿á¼°ÎºÎ¿Î´Î¿Î¼Î­Ï‰), (Î¿á¼°ÎºÎ¿Î´Î¿Î¼Î­Ï‰, Î±á¼°Ï‡Î¼Î±Î»Ï‰ÏƒÎ¯Î±), ...   \n",
       "381   [(ÏƒÏÎ¶Ï‰, Î»Î±ÏŒÏ‚), (Î»Î±ÏŒÏ‚, ÎµÏ…Ì“Î»Î¿Î³Î­Ï‰), (ÎµÏ…Ì“Î»Î¿Î³Î­Ï‰, ÎºÎ»...   \n",
       "1791  [(á¼€Î½Î¿Î¯Î³Î½Ï…Î¼Î¹, Î³á¿†), (Î³á¿†, ÎºÎ±Ï„Î±Ï€Î¯Î½Ï‰), (ÎºÎ±Ï„Î±Ï€Î¯Î½Ï‰, Î´...   \n",
       "\n",
       "                                               trigrams  \\\n",
       "1709  [(Î¿á¼¶Î½Î¿Ï‚, ÎµÏ…Ì“Ï†ÏÎ±Î¯Î½Ï‰, ÎºÎ±ÏÎ´Î¯Î±), (ÎµÏ…Ì“Ï†ÏÎ±Î¯Î½Ï‰, ÎºÎ±ÏÎ´Î¯...   \n",
       "2445  [(Î¼Î±ÎºÎ±ÏÎ¯Î¶Ï‰, Î»Î±ÏŒÏ‚, Îµá¼°Î¼Î¯), (Î»Î±ÏŒÏ‚, Îµá¼°Î¼Î¯, Î¼Î±ÎºÎ¬ÏÎ¹Î¿Ï‚...   \n",
       "1588  [(Î¿á¼¶ÎºÎ¿Ï‚, Î¿á¼°ÎºÎ¿Î´Î¿Î¼Î­Ï‰, Î±á¼°Ï‡Î¼Î±Î»Ï‰ÏƒÎ¯Î±), (Î¿á¼°ÎºÎ¿Î´Î¿Î¼Î­Ï‰, Î±...   \n",
       "381   [(ÏƒÏÎ¶Ï‰, Î»Î±ÏŒÏ‚, ÎµÏ…Ì“Î»Î¿Î³Î­Ï‰), (Î»Î±ÏŒÏ‚, ÎµÏ…Ì“Î»Î¿Î³Î­Ï‰, ÎºÎ»Î·Ï...   \n",
       "1791  [(á¼€Î½Î¿Î¯Î³Î½Ï…Î¼Î¹, Î³á¿†, ÎºÎ±Ï„Î±Ï€Î¯Î½Ï‰), (Î³á¿†, ÎºÎ±Ï„Î±Ï€Î¯Î½Ï‰, Î´Î±Î¸...   \n",
       "\n",
       "                                                    pos  \\\n",
       "1709  [coordinating_conjunction, noun, verb, noun, n...   \n",
       "2445  [verb, determiner, noun, pronoun, adjective, a...   \n",
       "1588  [subordinating_conjunction, determiner, noun, ...   \n",
       "381   [verb, determiner, noun, pronoun, coordinating...   \n",
       "1791  [verb, determiner, noun, coordinating_conjunct...   \n",
       "\n",
       "                                                 morpho  \\\n",
       "1709  [[], [(nominative, accusative, ergative, absol...   \n",
       "2445  [[(habitual, imperfective, iterative, perfecti...   \n",
       "1588  [[], [(nominative, accusative, ergative, absol...   \n",
       "381   [[(habitual, imperfective, iterative, perfecti...   \n",
       "1791  [[(habitual, imperfective, iterative, perfecti...   \n",
       "\n",
       "                                             vocabulary  \n",
       "1709  [ÎµÏ…Ì“Ï†ÏÎ±Î¯Î½Ï‰, ÎºÎ±ÏÎ´Î¯Î±, Î¿á¼¶Î½Î¿Ï‚, Ï€ÏÏŒÏƒÏ‰Ï€Î¿Î½, ÏƒÏ„Î·ÏÎ¯Î¶Ï‰, ...  \n",
       "2445  [Î±á½Ï„ÏŒÏ‚, Îµá¼°Î¼Î¯, Î¸ÎµÏŒÏ‚, ÎºÏÏÎ¹Î¿Ï‚, Î»Î±ÏŒÏ‚, Î¼Î±ÎºÎ¬ÏÎ¹Î¿Ï‚, Î¼Î±...  \n",
       "1588  [Î±á¼°Ï‡Î¼Î±Î»Ï‰ÏƒÎ¯Î±, Î³á¿†, Î´Î±Ï…Î¹Î´, ÎºÎ±Î¹Î½ÏŒÏ‚, ÎºÏÏÎ¹Î¿Ï‚, Î¿á¼°ÎºÎ¿Î´Î¿...  \n",
       "381   [Î±á¼°ÏÎ½, Î±á½Ï„ÏŒÏ‚, ÎµÏ…Ì“Î»Î¿Î³Î­Ï‰, ÎºÎ»Î·ÏÎ¿Î½Î¿Î¼Î¯Î±, Î»Î±ÏŒÏ‚, Ï€Î¿Î¯Î¼...  \n",
       "1791  [Î±Î²Î¹ÏÏ‰Î½, Î³á¿†, Î´Î±Î¸Î±Î½, ÎºÎ±Î»ÏÏ€Ï„Ï‰, ÎºÎ±Ï„Î±Ï€Î¯Î½Ï‰, ÏƒÏ…Î½Î±Î³Ï‰Î³...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# extract Ps verses from dataframe NT\n",
    "Ps = NT[NT.verse.str.contains(\"Ps\")].reset_index(drop=True) \n",
    "Ps = Ps[Ps.verse.str.contains(\"PsSol\")==False].reset_index(drop=True) # Remove PsSol verses\n",
    "display(Ps.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "baa1f079-acc4-4565-841b-fd9b15da9458",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/Ps.pkl\", \"wb\") as file:\n",
    "    pickle.dump(Ps, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831387d1-2e8c-4304-9aa5-4e53dab93190",
   "metadata": {},
   "source": [
    "## CSV export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "30cd9f55-99e7-4040-ac48-df344841a9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# compression_opts = dict(method='zip',archive_name='NT_lemmatized.csv')\n",
    "NT_simple = NT[['verse','text','tokens','lemmata','pos']]\n",
    "\n",
    "for i in range(NT_simple.shape[0]):\n",
    "    lemmes = ''\n",
    "    for j,l in enumerate(NT_simple.lemmata[i]):\n",
    "        lemmes = lemmes+' '+l\n",
    "    NT_simple.lemmata[i] = lemmes\n",
    "\n",
    "\n",
    "NT_simple.to_csv('NT_lemmatized.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660e70c5-7a91-4def-8344-6732b7e8b615",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP-env",
   "language": "python",
   "name": "nlp-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
